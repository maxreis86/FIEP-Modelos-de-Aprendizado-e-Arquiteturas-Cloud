{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 9.765238,
     "end_time": "2021-04-22T18:42:42.235677",
     "exception": false,
     "start_time": "2021-04-22T18:42:32.470439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install h2o_pysparkling_3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pysparkling.ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS\n",
    "# 3.36.1.3-1-3.1\n",
    "# 3.1.1+amzn.0\n",
    "\n",
    "# LOCAL\n",
    "# '3.36.1.3-1-3.1'\n",
    "# '3.1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark' from 'c:\\\\program files\\\\python37\\\\lib\\\\site-packages\\\\pyspark\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BSGB07T:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkContext</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x139c1f84dc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[2] pyspark-shell\"\n",
    "findspark.init('c:\\\\program files\\\\python37\\\\lib\\\\site-packages\\\\pyspark')\n",
    "sc = SparkContext('local', 'SparkContext')\n",
    "spark = SparkSession.builder.appName(\"SparkSession\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar conexão com o Athena\n",
    "my_boto3_session = boto3.Session(region_name='us-east-1',\n",
    "    aws_access_key_id='AKIATSOWRPN4647AE5NC',\n",
    "    aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data source to simulate as Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Lufa Lufa 2651 (ZigPay - QA Engineer) - Lari Gonçalves\n",
    "# - Crewmates 2665 (GPA _ Product Owner) - Batata\n",
    "# - Wildcats 2645 (Gabriel - Mobile Software Eng.) - Peu Gomes \n",
    "\n",
    "# 2546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# query = \"\"\"select trim(linkedin_user_name) as linkedin_user_name\n",
    "# from \"prod-lakehouse-mirror\".talents\n",
    "# where talent_status = 'AVAILABLE'\n",
    "# and linkedin_user_name is not null\"\"\"\n",
    "# lista1 = wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result').values.tolist()\n",
    "# flat_list = [item for sublist in lista1 for item in sublist]\n",
    "# flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro_talents = 'adewagold', 'adielcristo', 'adieztorio', 'adilio-santos-3ab5574a', 'adilson-a-krueger-a260562b', 'adilson-baggio', 'adilson-cardoso-barbosa-bb039234', 'adilson-krasovski-a5917353', 'adilson-matteucci-junior-31a8925b', 'adilson-moreira-silva-34964b7', 'adilson-muriel-8a9018163', 'adilson-quiozine-657692b', 'adilson-sierpinski-jr-95358b77', 'adilsonssantos', 'adine-laís-amorim-652007170', 'adiosmatheus', 'adlany-menezes-80b999218', 'adlerdias', 'adlerparnas', 'administradorvendas', 'admiurysouza', 'admonteirojr', 'adnan-ali-297b208a', 'adnangonzaga', 'adolfhoathyla', 'adolfo-agnelli-scolastico-15550b226', 'adolfo-menezes-silva', 'adolfo-ramos-8bb542162', 'adolfoabcruz', 'adolfoquaranta', 'adonai-silva-85b462105', 'adonay-puszczynski', 'adonega', 'adrazevedo', 'adrelaynesouza', 'adrian-dias-8b4647221', 'adrian-f-2a3a7536', 'adrian-fidelis-00223537', 'adrian-salomon-ferreira-abdesalan', 'adrian-sanabria', 'adriana-alcantara-7b11a622', 'adriana-almeida-091444103', 'adriana-almeida89', 'adriana-bicho', 'adriana-cabral-da-silva-a2a21124', 'adriana-carvalho-aaa0958b', 'adriana-coimbra-445474141', 'adriana-corrêa-86a1a133', 'adriana-coutinho-b4948951', 'adriana-cunha-a858421b', 'adriana-forcinetti-660b95133', 'adriana-freires', 'adriana-gomes-0890a231', 'adriana-h-8b29921', 'adriana-l-franco', 'adriana-lopes-pmp-b8079215', 'adriana-maciel-2963ab39', 'adriana-malta-3a339441', 'adriana-maria-de-almeida-oliveira-bbba84119', 'adriana-menezes-45822024', 'adriana-monteiro-373739117', 'adriana-muniz-andrade-34a24b15a', 'adriana-ribeiro-5a2b2914b', 'adriana-rosa-0243b2', 'adriana-sarmento', 'adriana-schneider-c-lima-256b8a81', 'adriana-scrobote', 'adriana-siqueira-lopes-b06369a', 'adriana-trindade-b33164a8', 'adriana-vianna', 'adrianaaraujocastagnino', 'adrianabloch', 'adrianabuenodasilva', 'adrianacostagallego', 'adrianadelimadossantos', 'adrianadelneroromano', 'adrianamegoncalves', 'adrianaparacencio', 'adrianaunger', 'adriane-da-hora-37a5ba13b', 'adriane-eckmann-mingrone-4656227b', 'adriane-leszczynski', 'adriane-s-45436220', 'adrianegomes', 'adrianemanhaes', 'adriann-murillo', 'adriano-amaral-29465326', 'adriano-barbieri-882603150', 'adriano-buniotti', 'adriano-carvalho-6a9987a5', 'adriano-correa-39174956', 'adriano-costa-lima', 'adriano-costa-tobias-783104a1', 'adriano-dias-4aba90161', 'adriano-espim-mendonça-21b4545a', 'adriano-fernandes-razzini-129a805a', 'adriano-ferreira-73274843', 'adriano-franco-1b766685', 'adriano-frede-hauber-4a0ab839', 'adriano-gomes-2a980637', 'adriano-leite-bi', 'adriano-marqueze-6474031', 'adriano-mendonca', 'adriano-mota-8a278531', 'adriano-moura', 'adriano-nakagawa-09111338', 'adriano-negrão-66952a38', 'adriano-nq', 'adriano-nunes-de-moura-64559b50', 'adriano-oliveira-37aa3145', 'adriano-palomino-bb3486105', 'adriano-pereira-9b03b23a', 'adriano-porto-22a775186', 'adriano-ramalho-8381b5a2', 'adriano-rezena', 'adriano-ribeiro-0333a526', 'adriano-ribeiro-de-almeida-40550814a?originalSubdomain=br', 'adriano-rodrigues-57125a63', 'adriano-rosa-092b3730', 'adriano-s-b55627b3', 'adriano-salustiano', 'adriano-santos-02750747', 'adriano-santos-296545193', 'adriano-silva-a1961a92', 'adriano-silva-santos-687969121', 'adriano-souza-santos-735a6ba8', 'adriano-veiga-07348517', 'adrianoaclina', 'adrianobarbosasilva', 'adrianoborin', 'adrianocaria', 'adrianodlucca', 'adrianogodinho', 'adrianogrygonis', 'adrianohax', 'adrianoheissig', 'adrianokerber', 'adrianolauton', 'adrianolinordj', 'adrianolourenco', 'adrianolucianocandido', 'adrianoluro', 'adrianomartins9', 'adrianoobarbosa', 'adrianorang', 'adrianorib', 'adrianosouza1310', 'adriansteinstrasser', 'adriel-basílio-59233676', 'adriel-da-silva-grizotti-090549178', 'adriel-lins-maciel', 'adriel-preto-10a71a3', 'adriel-rodrigues-ruis-8a613a112', 'adriel40torres', 'adrieleamaral', 'adrielera', 'adrieli-baggio-jaskiw-44692822', 'adrieli-sales-a88053161', 'adrielle-alecrim-039431169', 'adrielle-krauchuki-586857101', 'adrielle-macedo-550661157', 'adrielleasfarias', 'adriellezerbeto', 'adrielli-oliveira-078532137', 'adrielli-prado-589249116', 'adriellucas', 'adriene-aparecida-antunes-814529218', 'adrijunkes', 'adriley-samuel-ribeiro-barbosa-4a1310153', 'adrisson-silva', 'adroaldo-pereira-2a9876140', 'adryanaportugal', 'adryane-rodrigues-a3273118b', 'adryel-ramos-9535a7183', 'adryell-batista-089b73a9', 'adryelle-lima', 'adryellhenry', 'adson-emanuel-devops','adson-souza-21b1493a', 'adsonarthur', 'advaitapatel', 'advluanaferreira3813', 'adyb-amback-2819a1161', 'adyel-papini-a28a25211', 'adão-aurélio-amorim-de-oliveira-1219aa9b','adão-de-oliveira-lima', 'aeciolima', 'aersouza', 'aerton-oliveira-79708993', 'afbmarquezini', 'affonso-dick-neto-015930109', 'affonsocarvalho', 'afmeirelles', 'afoliveirareal', 'afonso-celso-dutra-acauan-filho-71a80621', 'afonso-simao', 'afonso-simões-baa33818', 'afonsoavr', 'afonsohfdpaula', 'afonsolelis', 'afonsompp', 'afonsosouza', 'afranio-lucas-190a0560', 'afreitas', 'afsiebra', 'agabiabreu', 'agata-laís-silveira-', 'agatapiza', 'agathaalvarenga', 'agenor-e-oliveira-bb0b91168', 'agenor-kaulli-93b82a12a', 'agenor-neto-01b6a4158', 'agenor-vicente-da-silveira-filho-a3a3a634', 'ageu-gomes-82214966', 'ageu-macedo-406038191', 'aggarwalanant', 'agilistagiovannyesposito', 'agmachado', 'agnaldo-sobrinho-1688512b', 'agnaldo-teodoro-rodrigues-6ba630208', 'agnaldo-venâncio-019bb6b3', 'agner-manzini-de-sousa-3a6870a2', 'agnes-tamás-76713050', 'agni-mendoza-68249a125', 'agomuodavid', 'agostinho-sieczkowski', 'agostinhobarone', 'agrarcanjo', 'agripino-neto-8aa20659', 'agt-infraestrutura-7099a2176', 'aguiarandre', 'aguilarrd', 'aguinaldo-brito-bbb184171', 'aguinaldo-teixeira-06734a4', 'aguinaldo-toledo-jr', 'agustin-gabriel-aguirre-569b6720b', 'agustin-gonzalez-nicolini-☁', 'agustina-alurralde-9b642ab2', 'agustina-cabral', 'agustinfogliatto', 'agustinho-coelho-97919a29', 'ahmed-nassif-a56b8527', 'ahmedkleit', 'aiane-bandeira', 'aida-gabriela-ghergus-873934117', 'aidastockler', 'aidm', 'aijalon-junior-6299551a5', 'ailson-pereira-539349183', 'ailton-alves-ba03b754', 'ailton-araujo-529a5722', 'ailton-araújo-júnior', 'ailton-mizuki-sato-27a21851', 'ailton-oliveira-127366126', 'ailton-santos-fh', 'ailton-tim', 'ailton-vinaud-júnior-356138101', 'ailton-vinicius-leite-dos-santos-49001b89', 'aimeefernandes1990', 'airesdiogo', 'aironprado', 'airton-júnior-machado-775801100','airton-l-00621117b', 'airton-lira-29045b26', 'airton-mendonca-silva', 'airton-miranda-13731a56', 'airton-neto93', 'airton-takeo-fukumoto-03a51219', 'airton-tibana-11434755', 'airtonlimajr', 'aisha-serrat-7193581b1', 'ajmemilio', 'aka-luan', 'akanjo', 'akegiraldo', 'akfzambrana', 'akiyamav', 'akizar-hercilia-sampaio-08701411a','akshaya-ramesh-08409b102', 'akshayagrawal87', 'alaa-mansour', 'alabvix', 'alain-green-mayoral', 'alamocr', 'alan-abreus', 'alan-alves-silva', 'alan-amadei-marciano-22662a38', 'alan-andrade-silva-b3382810b', 'alan-carlos-42028770', 'alan-cecchin-55771232', 'alan-costa-84778b120', 'alan-douglas-b4bb6066', 'alan-evangelista-a9968915', 'alan-ferreira-borba-891838a2', 'alan-moraes', 'alan-nonato-silva', 'alan-p-546346115', 'alan-p-heleno', 'alan-pascoli', 'alan-r-542324a9','alan-rocha-08651656', 'alan-rodrigo-f-c-guimaraes', 'alan-sampaio-a44804168', 'alan-sousa-1a1a91113', 'alan-souza-5709809', 'alan-souza-963806162', 'alan-sérgio-victor-472b78b3', 'alan-torres-es', 'alan-weingartner', 'alan-william-pmp', 'alan-willy-leiser-800274195', 'alana-amaral', 'alana-costa-230b9ab0', 'alana-furlan-13587250', 'alana-gouveia-02693b24', 'alana-monteiro-56031a1b8', 'alana-rafaela-cardoso-de-brito-a00365181', 'alana-santana-75ba43208', 'alana-santiago-061799232', 'alana-vieira-28424359', 'alanarakaki', 'alanbittencourt', 'alandamatta', 'alandjansenrodrigues', 'alane-gomes-37b6a219a', 'alanfernandeslobao', 'alangaiovis', 'alangoncalves', 'alanives', 'alanmalbos', 'alanmartini', 'alanmattos', 'alanmeira', 'alanmoraesaai', 'alanna-azevedo-santa-rosa-67433b93', 'alanna-marzola-112b94146', 'alannalimadossantos', 'alanpita', 'alanpr', 'alansarda', 'alanscardoso', 'alanwillms', 'alaoolusola', 'alaricosong', 'alberico-bezerra', 'alberson-pains-57138029', 'albert-arasaki-1a365025', 'albert-domingues', 'albert-emiliano-19a0a7b6', 'albert-ferreira-289683157', 'albert-kolberg-399969a5', 'albertdiniz', 'alberteije', 'alberthycoelho', 'albertjose', 'alberto-abrisqueta-dengra-b87185138', 'alberto-baek', 'alberto-baxauli-504194', 'alberto-coli-a81610154', 'alberto-ferreira-araujo-352744b1', 'alberto-linard-77a87620', 'alberto-nozawa', 'alberto-oliveira-barbosa', 'alberto-piva-0a7914229', 'alberto-rosa-39001660', 'alberto-ruy-marques', 'alberto-silva-jr', 'alberto-sousa-de-santana-a6891677', 'alberto-techera-4494736', 'alberto-telles', 'alberto-velez-trejo-669636169', 'albertolanca', 'albertonasc', 'albertoogura', 'albertoolopes', 'albertotrevezani', 'albertulhoatimo', 'albino-meneguetti-neto', 'albinosv', 'albsilva0', 'albtsantos', 'albuquerque-alan', 'albuquerque-bruno', 'albuquerque22brunoo', 'albuquerqueandressa', 'albuquerquedeveloper', 'albuquerqueflavio', 'alcantara-thiago', 'alcarsan', 'alcedirjunior', 'alceu-joaquim-7a128965', 'alceu-morais-jr-06805925', 'alcides-campos-a4aaa432', 'alcides-monteiro-24483236', 'alcides-oliver-sencio-paes-096a7921', 'alcion-souza', 'alcione-dias-da-silva-06594029', 'alcione-martins-b304479a', 'aldaalmeidatech', 'aldeba', 'aldeir-francisco-da-silva-657801192', 'aldeir-maciel-63ba54b8', 'aldemircamara', 'aldenir-gil-de-oliveira', 'alder-furtado', 'aldo-rovani-santos-45359a24', 'aldocruz9', 'aldori', 'aldrea-malheiros-oliveira-rabelo-b5950325', 'aldridgeneto', 'aldutra', 'ale-charif-a367a752', 'aleandro-vicente-1306a0189', 'aleanyzewski', 'alecindro-castilho-b7620652', 'alecnsilva', 'alecsandro-augusto-cararo-7b62a3b', 'alecssander-machado-972586164', 'alef-alves-lemos-0815521b0', 'alef-cordeiro-62678099', 'alef-lopes-723404125', 'alefabian', 'alefalcantara', 'alefcampos', 'alefdav', 'alefe-sousa', 'aleff-remberto-0b60b21b2', 'aleff-stampini-10761b186'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro_jobs = 2689, 2703\n",
    "# filtro_jobs = 2689, 2546, 2666, 2214, 2578, 2345, 2303, 2454, 2222, 2645, 2651, 2665, 2671, 2669, 2449, 2675, 2267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36 minutes 32 seconds : 2703 2689 2680 2675 2671 2669 2666 2665 2651 2645 2578 2546 2454 2449 2345 2303 2267 2222 2214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48 minutes 14 seconds: 2689, 2546, 2666, 2214, 2578, 2345, 2303, 2454, 2222, 2645, 2651, 2665, 2671, 2669, 2449, 2675, 2267, 2703, 2680, 2693, 2692, 2689, 2687, 2685, 2684, 2683, 2681, 2680, 2679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hour 6 minutes 45 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "select\n",
    " job_id\n",
    ",company_id\n",
    ",job_seniority\n",
    ",case when max_salary_offered is null then cast('Missing' as varchar) else cast(max_salary_offered as varchar) end as max_salary_offered\n",
    ",case when min_salary_offered is null then cast('Missing' as varchar) else cast(min_salary_offered as varchar) end as min_salary_offered\n",
    ",job_area\n",
    ",job_status\n",
    ",job_department\n",
    ",job_position\n",
    ",job_hiring_type\n",
    ",company_classification\n",
    ",case when import_policy is null then 'Missing' else import_policy end as import_policy\n",
    ",case when job_validation_questions is null then 'Missing' else job_validation_questions end as job_validation_questions\n",
    ",job_credits\n",
    "from \"prod-lakehouse-mirror\".jobs\"\"\"\n",
    "datasource_jobs0 = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_jobs0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select *\n",
    "from \"prod-lakehouse-mirror\".jobs_questions\"\"\"\n",
    "datasource_questions = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_questions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select *\n",
    "from \"prod-lakehouse-mirror\".jobs_skills\"\"\"\n",
    "datasource_skills = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_skills.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select *\n",
    "from \"prod-lakehouse-mirror\".clients_offlimits\"\"\"\n",
    "datasource_clients_offlimits = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_clients_offlimits.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select company_id, linkedin_company_user_name, flag_off_limit\n",
    "from \"prod-lakehouse-mirror\".clients\"\"\"\n",
    "datasource_clients0 = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_clients0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select\n",
    " linkedin_user_name\n",
    ",application_id_backoffice\n",
    ",job_id\n",
    ",application_creation_date_ts\n",
    ",application_current_phase_active\n",
    ",application_status\n",
    ",application_hiring_model\n",
    ",case when reason_disapproval is null then 'Missing' else reason_disapproval end as reason_disapproval\n",
    ",case when talent_last_email is null then 'Missing' else talent_last_email end as talent_last_email\n",
    ",case when pipefy_last_card_id_link is null then 'Missing' else pipefy_last_card_id_link end as pipefy_last_card_id_link\n",
    ",expected_salary_interval\n",
    ",max_salary_expected\n",
    "from \"prod-lakehouse-mirror\".applications\n",
    "where linkedin_user_name is not null\"\"\"\n",
    "datasource_applications = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_applications.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select\n",
    " linkedin_user_name\n",
    ",approach_id\n",
    ",job_id\n",
    ",created_at_date_ts\n",
    ",approach_origin\n",
    "from \"prod-lakehouse-mirror\".approaches\n",
    "where linkedin_user_name is not null\"\"\"\n",
    "datasource_approaches = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_approaches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista2 = datasource_approaches.select('approach_id').toPandas().values.tolist()\n",
    "# flat_list = [item for sublist in lista2 for item in sublist]\n",
    "# flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro_approaches_status = '19f226d5-e7b8-4948-aa0a-f24c76c347a6', '042e3f96-2f54-41ac-9e6d-3bdaf7c0002a', '0910f3c2-2cf3-4e3e-8508-82f5583e44ad', '1862ae28-cfc8-4d0f-9403-746e6a8d05e5', '4f6e771d-26c6-4ff0-a1f2-85f5733321f3', '57e21188-596a-4f64-b48d-1ebd5a3c6483', '5faabcb3f67cb20cf1f93aa2', '5fbd74b8afa16f5050934d69', '601a9e231875ce48fd829aa4', '604283ee8051615e0df76e4b', '607d97358051615e0df85517', '60c1488b6a434811aa9cc0a7', '8bea80ff-fcc2-4391-bfa5-dfd36b39ae92', '924fa0e5-6ddc-459e-9dd1-d30ddb84947e', 'c512f6ca-d94a-4d9c-9881-4935002ed2ed', 'ca78949f-8081-493c-b2e4-446a50efafd5', 'f20a6613-a6b0-4397-9066-847e6757b39c', 'f7c94f73-f052-47a5-bed1-90837d260c02', 'fdbc3f62-432e-49cb-9d59-399b8fbac7a7', '2eb969a0-ba7f-48f0-9445-0f2282446478', '3337e7a5-3233-42c9-a308-136100299f44', '57483595-46b3-49fc-9dad-12372bb01eb4', '5f28778c2be57767ce1b3ea6', '5ff4b8f69ad81349da7844f2', '605a3b9c8051615e0df7d46f', '60622f328051615e0df7f470', '60638dcd8051615e0df7fce2', '60a2d146bca00c222682fce3', '60b6bb5d6a434811aa9c9cfd', '60de1a5939f7712b9e9848d8', '60f736b0f92b831e618f7a1e', '6daf5a11-3223-4d05-9d51-a5d7d9f160ba', '7997ee7f-fd38-4dfe-a5fa-f53ebf076431', '7dc00067-f080-40fc-ba6a-655c07b0743b', '86818d0b-a001-41a4-83c0-4be7bad6865a', '9f0ac5e4-b73e-4935-972d-f472743f1404', 'a20f7fda-572d-46aa-8dfb-aa64465cc3b0', 'a3630827-ac3d-401b-b9d6-d3e186b07e3a', 'a56fc955-2313-4f85-9404-0ed9e8c8d370', 'bd11f3e2-c8be-45c4-84b2-476b4ea020fa', 'eb974535-a5b5-4d2d-9af3-d6d810ff107a', 'f20e7a2e-e7f0-42bd-8f08-7be17b5837a7', '1cb049f1-134c-4f0a-a89a-17d682ac513d', '1f46ccfc-5e4e-445f-b5db-061166605ed2', '2c908af5-05ba-4754-bdb7-07253b9796fd', '4922a73d-383f-40f0-841e-bca2b315b30e', '4c112dec-7ad8-4d6e-9552-a14cd0edbc86', '5ee7ce42bd3c2010ffcaf212', '5fc6a260e600af76c344731f', '5ff61f1c-ece0-4805-af2b-357c90045f7d', '5ffedc6f6baded1fa7403743', '603426ea496a6573c0377722', '60535e0d8051615e0df7b4d1', '6075aa7c8051615e0df83495', '60ca17e86a434811aa9cdd0c', '61f798f6-8d7f-4d47-86e4-859d038480ca', '73036b0a-b53c-492d-a667-e99c15cdfc67', '762c6620-1efd-403d-8beb-6ef802ab3044', '8a4c3fb4-c3bd-4a9d-8a8a-f8f58f8fcea7', 'a5558c4d-b286-41f2-9766-25003d9ab6dd', 'bc37edca-adc9-49b8-b139-6ad90f08aa0f', 'ccfa1dff-07a7-4338-841b-17a3f019e8b3', 'e1a5b64c-cc50-4e25-b74d-dcf3b3eafa0b', '076b2b29-60dc-4b33-ba4d-0b1f8a4e7528', '1f94391d-fabf-434d-863a-431c78ee0a5b', '21fc503a-c668-4276-a923-0c1388a81f48', '2a714cc7-c763-44c4-93ed-e12d63f488a3', '3aeb3fc2-e932-4f8a-a2a0-bf3fded0f8de', '3fb2068e-5104-4315-a091-9fe70b78baea', '4d540856-74fa-4bf0-a515-1c0a628c5f3c', '5fa94dbcf67cb20cf1f932de', '5fe0ba38ac47d56bbcae2ba8', '60103a306baded1fa74064c9', '602687cc496a6573c037374c', '6054be1f8051615e0df7bde5', '605a44268051615e0df7d4f1', '60e753f15afd7d4eccc8f3a5', '6104582505d0c8523bae6c66', '612e753be6bb4a0407502924', '7fac5dc4-5663-4a3e-966c-71a50710f017', '8c613181-0fce-4295-aa6c-b4e2a1103fce', 'a684bd38-158b-4824-a615-93f0327513f8', 'a83070a9-8879-40b1-911b-ee0644bc192f', 'b18d25ac-50b6-410f-a63a-a7baeeea685d', 'dcc56cd7-a949-47b2-b1a8-c5844415d1e7', 'fed33536-def6-4fad-b213-073e5969ac09', '071b33bf-05bc-4a77-80c9-0cb2e011041c', '5faac9bff67cb20cf1f93ae7', '607a0efd8051615e0df8522f', '6123b5d105d0c8523bb9461d', '612e6b71e6bb4a0407501edb', '718b9caf-bc42-4c2c-bdb5-77802671d955', '94e407ec-7a91-42e4-9365-7f1c3297cf91', '9954a548-0f56-42ac-8f65-8d934d8532ba', 'a3e7cfbf-b205-4338-b523-b12c9de72af2', 'ae708173-ab81-4eee-959b-444deecb979d', 'b70c92c9-886f-48f0-a0b7-02a6e5b05560', 'bcaca3dd-13d7-48c7-95e5-750cf12c1c7e', 'e8ddf33d-b9d4-4d64-b465-ea4412a761b1', 'ffd9e114-a0b9-4c48-a5a1-3c2148e0b551', '67dc349b-d506-4fb5-97b5-8dc545ed640d', '21097153-4573-4dd7-87c8-2f43f98c48de', 'bb7320fa-fb63-4f91-b74f-9ae2d45667cf', 'eb22676f-84d8-44bf-8766-aadd831a8a39', '6a81f24e-ed0b-4d26-855a-ffdf646b695d', '49214f75-cd50-4d1f-9de3-e7c9d3ad4ecd', '047a9ffc-bf4b-4429-b935-878ae8deb934', '1bc75564-54db-4c6f-b250-ec33b1cae8a3', '5482d67b-8b2b-4fbc-8911-175bc7f0011b', '5f3da61b710abf2110f8aa74', '5f6bcab111aa4d5c9ba500a3', '5f7e05f4c2982359d90e3847', '5f9c62e390f590307ad61a8a', '5fa93e63f902a23d56f06585', '5fd37dcc1228024f20cdd8de', '604a26c98051615e0df78c1d', '60520cfd8051615e0df7abbb', '606f16aa8051615e0df822fb', '6093017d9283bf5d769743f7', '609b3b01441c283a256a0daf', '60bf6f776a434811aa9cb08b', '614cb3f1e6bb4a040757be62', '62af8ea7-8a5e-4a19-9868-29544fa8ba79', '82cf4d9f-24c9-4210-8dfa-e8e7bee5f3b5', 'b8a615a5-176e-4d55-8b24-4ff9a079a067', 'c8034a34-385b-454f-aeac-033dee5f79a7', '066bfc79-38fd-4d2b-993c-5c61b17db967', '41cee8d7-dd85-40ab-b668-4ea083c1b301', '48fc42b9-5d65-4eb3-a2ec-72597c307693', '59c900ed-8ceb-4cef-a27a-3955bad9758a', '5a6a60f3-8f49-4090-bfe5-6f44558c8e1d', '5f622b90567aeb49736976ee', '601c1d658ae05821a7b3b77a', '603fe68e8051615e0df75f70', '60abec55a412c030cfe4ca41', '60c90c44-3689-4b80-a624-dea8a3add5a5', '60d5c7476a434811aa9d0e46', '690e3089-4e34-4649-8d26-1a7da4bafb64', '81d38132-eea4-4267-907f-00579caa4743', '9ae92875-7496-4326-a414-0da9f51cabbc', 'a2f37302-7ed6-40ed-9869-a766d490d2d4', 'b5ac93a6-3d7d-45e1-b8b7-08aba9eee135', 'ea8a1d1d-150c-4398-bbf0-f44d2190b94c', '139f3f6c-bf6c-4db2-b195-f1bea9c48f34', '1969b379-7046-48c9-862c-6ed245b80c14', '1a4d9f51-fc35-4911-9ef1-c16a00fc8937', '522fc1c1-0cd5-4d6f-b7df-a97b472b9100', '52c128b7-3949-473b-a0a6-5c98220fe39f', '5a5ff1d1-74e6-4cc1-a6a9-e16d372c3a50', '5f6ba9ec11aa4d5c9ba4ff98', '60a2e111bca00c222682fd7d', '610a961205d0c8523bb0297d', '612cec20e6bb4a04074f9ee2', '613f4ca9e6bb4a0407538d5e', '66854506-50c9-45b1-b655-febe7bab6785', '9212be85-5482-4496-92b7-dbe6ec4b824f', '9c61bc4f-9c2b-410b-b683-ff913e7b249f', 'b5718126-9403-4caa-932c-d2c09a23f162', 'd47a0aa4-039e-4a8c-b94c-186c16946232', 'f092fd5b-d5d1-4718-a45a-6d2259cd99c0', '00766c12-7750-4e8c-a08a-b6659900c959', '4646bd41-6952-4c0d-b435-f9ab58184c97', '46ba6a7e-1cfa-4bf4-a975-23f8ee735b2d', '489d0bb5-02ed-4354-a3e6-5b2a1b617142', '5f77622f992e700610b9080b', '605a37aa8051615e0df7d439', '6064ec9c8051615e0df8054c', '60aba26bbca00c2226832039', '60f0784cdd9c422acfac9571', '60f82038f92b831e618fd1d8', '613ba108e6bb4a040753453a', '680750c0-168f-40d4-b5d8-6143de7dc1b3', '6c2dce94-6b14-4823-97a0-28bc24ae150b', '913973c9-fc0d-49c4-bf42-1af78c7b85c6', '975229d2-7260-4e90-925e-a2b0f66446af', 'aaa47601-fb97-4eaa-9f3c-a5f998938235', 'b58591e4-a395-497b-b2d0-d9a4c16931c9', 'd52aefae-243a-4ca5-aff8-c8f006c621e9', 'e4688a55-2ec2-49d2-9942-a552dc4165f2', 'f0cfe5ec-2dca-41ed-9d12-587fe83e8eb6', '13911e40-3d8a-4abb-9d21-1f92a6c6f428', '177416ee-726b-4d58-9d66-c69edd546875', '5419024c-d9a2-4326-91e7-a0e86f4f678a', '580c3999-370a-4330-bc47-9182dda4fbda', '5cfb8866-7869-49da-8b27-8904f77de4ac', '5f71ed862393a85582261490', '5fd120969df0f90f69ed481d', '6062044a8051615e0df7f24f', '607ef0518051615e0df860c0', '609e816f125bb714071e8415', '60e5f3e3d33b546b5d4f7046', '97b42a06-f1f0-46d2-9666-6f891aae5818', 'a577f8f6-6e4b-415a-b518-ee3c408517f9', 'dc28876b-7119-4161-bed1-aab803857a81', '10c8cb49-3797-48f2-9285-5e1275165f6c', '4e7fc406-0865-4077-a6b6-d92e8dc052e7', '5f21b4f32be57767ce1b3925', '5f4d24f39a830b4db264a466', '5faef5ebf67cb20cf1f94b2a', '5fe1248eac47d56bbcae2db0', '638875d6-4549-46bb-ab6d-a35136887810', 'a40137d0-9c5b-4461-88a0-bf342ffbae5e', 'af5ea37b-efac-423b-9cca-eec1bc9c15d5', 'b0406d4e-937a-4c83-bc26-38d3652c1f80', 'b1116f6d-d8ee-48e0-9cb1-a4bcfa33e91f', 'c5f23a9f-e535-4c4f-a130-bb7839560ad2', 'd2f9407b-ee06-4fd2-b80a-0bd7b8c93f08', 'd71ea461-11be-4963-a7aa-aa2a36bc2206', '17cf3b14-3037-442a-8790-ce3a53c6d11f', '27f00003-a7fa-4cab-afb6-6fd9196f1488', '368227d6-3ded-49e4-9d33-8f02f47435e0', '528f92d3-ef15-43a6-a99b-8e5c1d0a18e5', '55c659c2-9b38-4d37-8484-7cdc1bbfd0bd', '5c879b69-542b-4792-bd57-5646b5ad963d', '5f3d940d710abf2110f8aa4a', '5ff754009ad81349da784efb', '611abb4505d0c8523bb5eddc', '6130e394e6bb4a0407510e9e', '6eaf288d-4740-4e73-a58c-8011ce4a20e0', '93c8060e-b8a7-43f0-9a7f-161fe77bbb57', '9ae36236-60e0-4a87-ba5e-ed90f2878a41', 'a678d156-79ce-413d-aa7e-1b319360c519', 'd311ca8d-d7ef-4ea4-b36f-b312e2bca971', 'e7eecb74-80c1-4a44-a912-9a21d1c9f185', '02f9937f-6b19-4faf-95cc-f54160bfde07', '0386a4ad-6038-4efc-9925-998e19800ed1', '158d60ea-e367-44b0-8b51-daa5cb5ebf90', '260cd6f1-dda3-49e3-9384-dea67fa526da', '364cfdbc-db9f-46eb-9d53-240fb1a30380', '4d56a51d-c37f-401b-b7e2-7352ebd246b2', '5e14defe-eba4-4446-83eb-c26d42ae40ee', '5f3fd67e-4e48-4500-8dad-a9fafa0e25c0', '5fd911b4ac47d56bbcae1943', '604f64988051615e0df799f2', '607f10348051615e0df86249', '611ad49605d0c8523bb616b1', '61254909cc610b737fecfeb8', '6fa6028f-0da2-4042-aa2e-d3b9c214c400', '70ff95aa-bffa-459a-a86e-4fe6bf4398fd', 'c1e7da3f-8ea9-4f0f-a7eb-494c12dc7fd3', 'd0fb8ae1-6df0-489f-98cf-8ef20882a00d', 'ee32013f-a8d7-4dd3-9274-6a7ad640a062', 'ee39ae2e-1b6d-42c6-94b9-69c07a4512bf', '5b7a960c-a1b2-486b-91b7-805b3747faea', '5f58dc87567aeb49736968e1', '5f8d9c922829290f4e79c237', '5f9c729e90f590307ad61b0b', '5fc0244091fa6c6d79db9047', '5ffdd4b36baded1fa7403479', '601a7f341875ce48fd829a08', '604140468051615e0df7686b', '60d60e8ff6d89a7503b16fe0', '612e8746e6bb4a0407503e89', '612f8b76e6bb4a0407507ff5', '612fcfc3e6bb4a040750bac1', '67a98c02-1fd1-4a86-901f-6daedb76add1', '69aa99c4-2b03-43ab-8453-731bdb0cfded', '88446a56-2919-4262-972b-4501448eb36c', '9fad4917-a247-4534-8f8f-6590ff877da3', '9fdc1208-8eec-498b-986c-e7f4a4a98377', 'b282ccff-7bbe-4d01-9838-9dcc02900c77', 'bea50e75-bb8a-4aca-8149-45b5dfd7262a', 'c8de2ba6-d5a5-4098-ac0a-c98eed12011a', '2666646e-f99a-4b4e-9993-fac0862794a6', '2b07222a-a7b5-4ab0-930b-e79d921025aa', '2fcfda3c-91ba-4285-8f25-8c00cb2dc0d3', '3228cb45-e3f0-44ee-bdc4-c0e65282adf4', '370609f9-ae02-4ef5-92af-58dcccd87490', '3e2253c0-d1bd-4421-9b62-8bd9be0825b8', '44703acb-8e2c-4c34-8e89-2918455800ac', '516148c1-a582-41f3-ac72-54dcba9195bb', '54de2cc7-416f-4b2b-8a98-deecaca7f156', '56a0b295-5543-4356-be61-e669c86ccf4c', '5f2067f12be57767ce1b37a6', '5f7e4854c2982359d90e397c', '5fbe966eafa16f5050935038', '5fd911c6ac47d56bbcae1944', '607f16748051615e0df862ce', '60a6bec7bca00c222683162a', '60e79e26-5f95-46a1-afed-639340abeaf1', '6112e00c05d0c8523bb36721', '611d865305d0c8523bb78d10', '614a3474e6bb4a040756fac9', '65137c6b-db76-4b8b-b5d9-ad781339df97', '6d5535b0-3599-453b-8819-a76b2e7d5307', '71804a3d-81e6-41ec-8f2b-284e3841d56d', '74c056a8-f154-4fe6-a6a3-2be4d2392332', '82e5d092-cd84-4008-8bba-5dfa5f19a73d', '9ad97341-5524-4312-90f5-0836af88913b', 'd2912071-f154-49ab-acee-f2c3b7fb6b29', 'e2f315c2-2f57-4384-81d5-4fc46ec57844', 'f7f6014a-104a-4c62-b432-2b324c4ed7ff', '1534ee94-c90a-49a9-89c5-4483c2c9dcf3', '3fca6009-6d5c-4cb6-bfdc-b8b7ce98c0be', '404735ff-f9ef-4a17-8646-a46a4a09332a', '57b99beb-d9fc-4d77-af05-a87648791cd3', '5f889fc62829290f4e79bdfc', '5fc93b400d16412348b91780', '802cfa05-80a4-4db4-b762-8f2fc6175fe9', '814dae66-ea9a-430f-8c0b-6e3a5cae08f5', '81bd0e2b-a423-49b4-8d38-100c3e807f9c', '841ad90b-0540-47fe-b54d-0cfbd5ca33b0', 'b4b5c168-1e6a-4810-93aa-27245b0365cc', 'cb1f3f7b-3651-4b5c-baf6-8656a0c241e3', 'ccd49ce3-923f-4bdb-82d9-51b0e32a134d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select\n",
    " approach_id\n",
    ",approach_status\n",
    ",approach_status_date_ts\n",
    "from \"prod-lakehouse-mirror\".approaches_status\"\"\"\n",
    "datasource_approaches_status0 = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_approaches_status0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select\n",
    " linkedin_user_name\n",
    ",case when prospect_location is null then 'Missing' else prospect_location end as prospect_location\n",
    ",updated_at_date_ts\n",
    ",prospect_seniority\n",
    ",case when prospect_area is null then 'Missing' else prospect_area end as prospect_area\n",
    ",declared_seniority\n",
    "from \"prod-lakehouse-mirror\".prospects\n",
    "where trim(split(prospect_area,' - ')[1]) in (select distinct trim(job_area) from jobs where job_id in {filtro_jobs})\"\"\"\n",
    "datasource_prospects = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_prospects.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select a.*\n",
    "from \"prod-lakehouse-mirror\".prospects_smart_skills as a\n",
    "inner join \"prod-lakehouse-mirror\".prospects as b\n",
    "on a.linkedin_user_name = b.linkedin_user_name\n",
    "where trim(split(prospect_area,' - ')[1]) in (select distinct trim(job_area) from jobs where job_id in {filtro_jobs})\"\"\"\n",
    "datasource_prospects_smart_skills = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_prospects_smart_skills.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select a.*\n",
    "from \"prod-lakehouse-mirror\".prospects_diversity as a\n",
    "inner join \"prod-lakehouse-mirror\".prospects as b\n",
    "on a.linkedin_user_name = b.linkedin_user_name\n",
    "where trim(split(prospect_area,' - ')[1]) in (select distinct trim(job_area) from jobs where job_id in {filtro_jobs})\"\"\"\n",
    "datasource_prospects_diversity = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_prospects_smart_skills.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select\n",
    " a.linkedin_user_name\n",
    ",case when a.linkedin_company_name is null then 'Missing' else a.linkedin_company_name end as linkedin_company_name\n",
    ",case when a.experience_location is null then 'Missing' else a.experience_location end as experience_location\n",
    ",case when a.experience_position is null then 'Missing' else a.experience_position end as experience_position\n",
    ",case when a.experience_date_from is null then 'Missing' else a.experience_date_from end as experience_date_from\n",
    ",case when a.experience_date_to is null then 'Missing' else a.experience_date_to end as experience_date_to\n",
    ",case when a.experience_duration_months is null then 0 else a.experience_duration_months end as experience_duration_months\n",
    ",a.flag_last_experience\n",
    ",a.company_classification\n",
    ",case when a.linkedin_company_user_name is null then 'Missing' else a.linkedin_company_user_name end as linkedin_company_user_name\n",
    ",case when a.experience_description is null then 'Missing' else a.experience_description end as experience_description\n",
    "from \"prod-lakehouse-mirror\".prospects_experiences as a\n",
    "inner join \"prod-lakehouse-mirror\".prospects as b\n",
    "on a.linkedin_user_name = b.linkedin_user_name\n",
    "where trim(split(prospect_area,' - ')[1]) in (select distinct trim(job_area) from jobs where job_id in {filtro_jobs})\"\"\"\n",
    "datasource_prospects_experiences = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_prospects_experiences.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"select distinct a.linkedin_user_name, a.talent_status\n",
    "from \"prod-lakehouse-mirror\".talents as a\n",
    "inner join \"prod-lakehouse-mirror\".prospects as b\n",
    "on a.linkedin_user_name = b.linkedin_user_name\n",
    "and a.talent_status = 'AVAILABLE'\n",
    "and trim(split(b.prospect_area,' - ')[1]) in (select distinct trim(job_area) from jobs where job_id in {filtro_jobs})\n",
    "and a.linkedin_user_name in ('brunocorreiaa', 'renancambre', 'bscmv', 'luis-rodrigues-phd', 'thiago-nobre-mascarenhas-108a99105', 'patrickreis', 'viniciusamaro', 'msantino', 'gabrielstankevix', 'michelfornaciali', 'lirielly', 'rodrigo-diana', 'cicduarte', 'pfcor', 'maria-clara-vilas-boas-845115144', 'marinaldo-andrade-de-sousa-622b7a167', 'lazarini-bruno')\"\"\"\n",
    "datasource_talents = spark.createDataFrame(wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result'))\n",
    "# datasource_talents.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to Run on AWS Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue Job step by step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PathModelMojo='StackedEnsemble_BestOfFamily_4_AutoML_1_20220907_214850.zip'\n",
    "\n",
    "###\n",
    "# import sys\n",
    "# from awsglue.transforms import *\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "# from awsglue.context import GlueContext\n",
    "# from awsglue.job import Job\n",
    "# from pyspark.context import SparkContext\n",
    "# from pyspark.sql.types import *\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "# from pysparkling.ml import H2OMOJOSettings\n",
    "# from pysparkling.ml import H2OMOJOModel\n",
    "###\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "###\n",
    "# args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "###\n",
    "\n",
    "#Job setup\n",
    "###\n",
    "# sc = SparkContext()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "###\n",
    "# spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "#desabilitado a funcao de BroadcastJoin do spark porque estava ocorrendo erra de timeout 300 secs nos joins\n",
    "\n",
    "###\n",
    "# job = Job(glueContext)\n",
    "# job.init(args[\"JOB_NAME\"], args)\n",
    "###\n",
    "\n",
    "## selected Features: variaveis que entraram no modelo\n",
    "CAT = ['application_current_phase_active'\n",
    ",'application_hiring_model'\n",
    ",'expected_salary_interval'\n",
    ",'reason_disapproval'\n",
    ",'application_status'\n",
    ",'prev_application_job_area'\n",
    ",'prev_application_job_department'\n",
    ",'last_company_classification'\n",
    ",'job_area'\n",
    ",'prev_approach_job_area'\n",
    ",'prev_approach_status'\n",
    ",'company_classification_migration'\n",
    ",'talent_info_source'\n",
    ",'prev_approach_job_area_to_job_area'\n",
    ",'prev_application_job_area_to_job_area'\n",
    ",'prev_approach_company_classification_migration'\n",
    ",'prev_application_company_classification_migration'\n",
    ",'prev_approach_job_seniority_to_job_seniority'\n",
    ",'prev_application_job_seniority_to_job_seniority'\n",
    ",'prev_approach_job_position_to_job_position'\n",
    ",'prev_application_job_position_to_job_position'\n",
    ",'prev_application_job_hiring_type_to_job_hiring_type'\n",
    ",'prospect_seniority_migration'\n",
    ",'declared_seniority_migration'\n",
    ",'prospect_location_state'\n",
    ",'prospect_location_region'\n",
    ",'prospect_area_migration']\n",
    "\n",
    "#float\n",
    "NUM = ['previous_applications_qty'\n",
    ",'days_since_last_application'\n",
    ",'prospect_smart_skills_qty'\n",
    ",'prospect_experiences_qty'\n",
    ",'prospect_companies_qty'\n",
    ",'total_experience_months'\n",
    ",'experience_duration_months_min'\n",
    ",'experience_duration_months_clean_avg'\n",
    ",'last_experience_duration_months'\n",
    ",'max_salary_offered'\n",
    ",'previous_approaches_qty'\n",
    ",'max_salary_offered_to_prev_approach'\n",
    ",'max_salary_offered_to_prev_application'\n",
    ",'last_experience_descriptions_word_count'\n",
    ",'all_company_classifications_count'\n",
    ",'all_job_question_correct_answers_word_count'\n",
    ",'all_job_skill_names_word_count'\n",
    ",'import_policy_word_count'\n",
    ",'job_validation_questions_word_count'\n",
    ",'match_skill_count'\n",
    ",'days_since_last_approach']\n",
    "selected_features = CAT + NUM\n",
    "\n",
    "##functions\n",
    "def ratings(p1):\n",
    "    if p1 <= 0.0338300070001797:\n",
    "        return 1\n",
    "    elif p1 <= 0.0486186573387924:\n",
    "        return 2\n",
    "    elif p1 <= 0.0623809672934295:\n",
    "        return 3\n",
    "    elif p1 <= 0.0827157113995381:\n",
    "        return 4\n",
    "    elif p1 <= 0.1290387149453732:\n",
    "        return 5\n",
    "    elif p1 <= 0.1499471801108515:\n",
    "        return 6\n",
    "    elif p1 <= 0.1767854001417186:\n",
    "        return 7\n",
    "    elif p1 <= 0.2136274391366556:\n",
    "        return 8\n",
    "    elif p1 <= 0.2771891382694837:\n",
    "        return 9\n",
    "    else:\n",
    "        return 10\n",
    "ratings_func = F.udf(ratings, StringType())\n",
    "\n",
    "def suggestion(predict):\n",
    "    if predict == 0:\n",
    "        return 'Repensar'\n",
    "    elif predict == 1:\n",
    "        return 'Abordar'\n",
    "    else:\n",
    "        return 'SUGGESTION_ERROR'\n",
    "suggestion_func = F.udf(suggestion, StringType())\n",
    "\n",
    "def list_word_count(lista):\n",
    "    lista = ' '.join(map(str,lista))\n",
    "    if lista in ('', 'null', None, 'None'):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(lista.split())\n",
    "list_word_count_func = F.udf(list_word_count, IntegerType())\n",
    "\n",
    "def string_word_count(string):\n",
    "    if string in ('', 'null', None, 'None'):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(string.split())\n",
    "string_word_count_func = F.udf(string_word_count, IntegerType())\n",
    "\n",
    "def word_count(lista):\n",
    "        return len(lista)\n",
    "word_count_func = F.udf(word_count, IntegerType())\n",
    "\n",
    "def experience_word_count(string):\n",
    "    if string in ('', 'null', None, 'None', 'Missing'):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(string.split())\n",
    "experience_word_count_func = F.udf(experience_word_count, IntegerType())\n",
    "\n",
    "def company_count(lista):\n",
    "    count = lista.count('Missing')\n",
    "    for i in range(count):\n",
    "        lista.remove('Missing')\n",
    "    return len(lista)\n",
    "company_count_func = F.udf(company_count, IntegerType())\n",
    "\n",
    "def company_word_count(lista):\n",
    "    lista = ' '.join(map(str,lista))\n",
    "    if lista in ('', 'null', None, 'None'):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(lista.split())\n",
    "company_word_count_func = F.udf(company_word_count, IntegerType())\n",
    "\n",
    "#Criação das variáveis de pais com base na variavel propesct_location do linkedin. Para comparar abordagens de talentos que moram fora do Brasil\n",
    "def prospect_country(country):\n",
    "    if country in ('', 'null', None, 'None', 'Missing'):\n",
    "        return \"Missing\"\n",
    "    elif country == 'São Paulo':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Rio de Janeiro':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Campinas':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Belo Horizonte':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Porto Alegre':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Curitiba':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Brasília':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Florianópolis':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Salvador':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Fortaleza':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Recife':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Manaus':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Ribeirão Preto':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Goiânia':\n",
    "        return 'Brazil'\n",
    "    elif country == 'João Pessoa':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Londrina':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Vitória':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Cuiabá':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Greater São Paulo Area':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Natal':\n",
    "        return 'Brazil'\n",
    "    elif country == 'São luis':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Brazil':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Brasil':\n",
    "        return 'Brazil'\n",
    "    else:\n",
    "        return 'Others'\n",
    "    \n",
    "prospect_country_func = F.udf(prospect_country, StringType())\n",
    "    \n",
    "#Separacao entre os estados brasileiros, os estados do norte e nordeste foram definidos como um unico grupo porque o volume de abordagens eh menor\n",
    "def prospect_region(state):\n",
    "    if state in ('', 'null', None, 'None', 'Missing'):\n",
    "        return \"Missing\"\n",
    "    elif state == 'São Paulo':\n",
    "        return 'São Paulo'\n",
    "    elif state == 'Minas Gerais':\n",
    "        return 'Minas Gerais'\n",
    "    elif state == 'Rio de Janeiro':\n",
    "        return 'Rio de Janeiro'\n",
    "    elif state == 'Paraná':\n",
    "        return 'Paraná'\n",
    "    elif state == 'Santa Catarina':\n",
    "        return 'Santa Catarina'\n",
    "    elif state == 'Rio Grande do Sul':\n",
    "        return 'Rio Grande do Sul'\n",
    "    elif state == 'Espírito Santo':\n",
    "        return 'Espírito Santo'\n",
    "    elif state == 'Mato Grosso do Sul':\n",
    "        return 'Mato Grosso do Sul'\n",
    "    elif state == 'Mato Grosso':\n",
    "        return 'Mato Grosso'    \n",
    "    elif state == 'Goiás':\n",
    "        return 'Goiás'\n",
    "    elif state == 'Paraíba':\n",
    "        return 'Paraíba'\n",
    "    elif state == 'Pernambuco':\n",
    "        return 'Pernambuco'\n",
    "    elif state == 'Bahia':\n",
    "        return 'Bahia'\n",
    "    elif state == 'Sergipe':\n",
    "        return 'Sergipe'\n",
    "    elif state == 'Alagoas':\n",
    "        return 'Alagoas'\n",
    "    elif state == 'Ceará':\n",
    "        return 'Ceará'\n",
    "    elif state == 'Maranhão':\n",
    "        return 'Maranhão'\n",
    "    elif state == 'Piau':\n",
    "        return 'Piau'\n",
    "    elif state == 'Rio Grande do Norte':\n",
    "        return 'Rio Grande do Norte'\n",
    "    elif state in ( 'Acre'\n",
    "                   ,'Amapá'\n",
    "                   ,'Amazonas'\n",
    "                   ,'Pará'\n",
    "                   ,'Rondônia'\n",
    "                   ,'Roraima'\n",
    "                   ,'Tocantins'):\n",
    "        return 'Norte'\n",
    "    else:\n",
    "        return 'Others'\n",
    "    \n",
    "prospect_region_func = F.udf(prospect_region, StringType())\n",
    "\n",
    "#Correção da variavel estado quando o pais nao for Brasil\n",
    "def prospect_region_international(state, country):\n",
    "    if country not in ('Brazil', 'Missing'):\n",
    "        return \"International\"\n",
    "    elif state in ('', 'null', None, 'None', 'Missing'):\n",
    "        return \"Missing\"\n",
    "    elif state == 'sao paulo':\n",
    "        return \"São Paulo\"\n",
    "    elif state == 'Federal District':\n",
    "        return \"Distrito Federal\"\n",
    "    else:\n",
    "        return state\n",
    "prospect_region_international_func = F.udf(prospect_region_international, StringType())\n",
    "\n",
    "#Se a fonte dos dados do talento vieram de \"Approaches\", \"Applications\" ou \"Prospects\"\n",
    "def talent_info_source(previous_approaches_qty, previous_applications_qty, prospects_updated_at_date_ts):\n",
    "    try:\n",
    "        source = 'null'\n",
    "        try:\n",
    "            if prospects_updated_at_date_ts not in ('', 'null', None, 'None'):\n",
    "                if source in ('', 'null', None, 'None'):\n",
    "                    source = 'prospects'\n",
    "                else:                    \n",
    "                    source = '_'.join([source, 'prospects'])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if (previous_approaches_qty > 0):\n",
    "                if source in ('', 'null', None, 'None'):\n",
    "                    source = 'approaches'\n",
    "                else:\n",
    "                    source = '_'.join([source, 'approaches'])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if (previous_applications_qty > 0):\n",
    "                if source in ('', 'null', None, 'None'):\n",
    "                    source = 'applications'\n",
    "                else:\n",
    "                    source = '_'.join([source, 'applications'])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if source in ('', 'null', None, 'None'):\n",
    "                return 'Missing'\n",
    "        except:\n",
    "            pass\n",
    "        return source\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "        return 'Missing'\n",
    "talent_info_source_func = F.udf(talent_info_source, StringType())\n",
    "\n",
    "def match_skill_count(job_skills, talent_skills):\n",
    "    counter = 0\n",
    "    if (job_skills not in ('', 'null', None, 'None', 'Missing')) & (talent_skills not in ('', 'null', None, 'None', 'Missing')):\n",
    "        for i in job_skills.split(','):\n",
    "            if talent_skills.split(',').count(i) > 0 and i != 'Missing':\n",
    "                counter += 1\n",
    "    return counter\n",
    "match_skill_count_func = F.udf(match_skill_count, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2591"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## JOBS\n",
    "# datasource_jobs = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"jobs\", transformation_ctx = \"datasource_clients\", additional_options={\"mergeSchema\": \"true\"}).toDF()\\\n",
    "# .fillna(value=\"Missing\")\n",
    "\n",
    "datasource_jobs = datasource_jobs0.fillna(value=\"Missing\")\n",
    "datasource_jobs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2591"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "datasource_jobs2 = datasource_jobs.withColumn('import_policy_word_count', string_word_count_func(F.col('import_policy')))\\\n",
    ".withColumn('job_validation_questions_word_count', string_word_count_func(F.col('job_validation_questions')))\\\n",
    ".withColumn('min_salary_offered', F.col('min_salary_offered').cast(IntegerType()))\\\n",
    ".withColumn('max_salary_offered', F.col('max_salary_offered').cast(IntegerType()))\\\n",
    ".withColumnRenamed('company_classification', 'job_company_classification')\\\n",
    ".drop('import_policy', 'job_validation_questions').persist()\n",
    "datasource_jobs2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# datasource_questions = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"jobs_questions\", transformation_ctx = \"datasource_questions\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_questions2 = datasource_questions\\\n",
    ".withColumn('job_question_correct_answers', F.regexp_replace(F.concat(F.col('question'), F.lit(' '), F.col('correct_answer')), ',', '||'))\\\n",
    ".groupBy('job_id').agg(F.collect_list(F.col('job_question_correct_answers')).alias('all_job_question_correct_answers'))\\\n",
    ".withColumn('all_job_question_correct_answers_word_count', list_word_count_func(F.col('all_job_question_correct_answers')))\\\n",
    ".drop('all_job_question_correct_answers')\\\n",
    ".withColumnRenamed('job_id', 'job_id_tmp')\n",
    "datasource_questions2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.64 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1303"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# datasource_skills = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"jobs_skills\", transformation_ctx = \"datasource_skills\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_skills2 = datasource_skills.groupBy('job_id').agg(F.collect_list(F.col('skill_name')).alias('all_job_skill_names'))\\\n",
    ".withColumn('all_job_skill_names_word_count', list_word_count_func(F.col('all_job_skill_names')))\\\n",
    ".withColumn('all_job_skill_names', F.concat_ws(\",\", F.col('all_job_skill_names')))\\\n",
    ".withColumnRenamed('job_id', 'job_id_tmp')\n",
    "datasource_skills2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.57 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# datasource_clients_offlimits = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"clients_offlimits\", transformation_ctx = \"datasource_clients_offlimits\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_clients_offlimits2 = datasource_clients_offlimits.dropDuplicates(subset = ['company_id', 'client_offlimit_linkedin_user_name'])\n",
    "datasource_clients_offlimits3 = datasource_clients_offlimits2.groupBy('company_id').agg(F.collect_list(F.col('client_offlimit_linkedin_user_name')).alias('client_offlimit_linkedin_user_names'))\\\n",
    ".withColumn('client_offlimit_linkedin_user_names', F.concat_ws(\",\", F.col('client_offlimit_linkedin_user_names')))\n",
    "datasource_clients_offlimits3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 337 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datasource_jobs3 = datasource_jobs2.alias('df1')\\\n",
    ".join(datasource_skills2.alias('df2'),\n",
    "on = datasource_jobs2['job_id'] == datasource_skills2['job_id_tmp'],\n",
    "how = 'left').select('df1.*',\n",
    "'df2.*').alias('df3')\\\n",
    ".join(datasource_questions2.alias('df4'),\n",
    "on = datasource_jobs2['job_id'] == datasource_questions2['job_id_tmp'],\n",
    "how = 'left').select('df3.*',\n",
    "'df4.*').alias('df5')\\\n",
    ".join(datasource_clients_offlimits3.alias('df6'),\n",
    "on = datasource_jobs2['company_id'] == datasource_clients_offlimits3['company_id'],\n",
    "how = 'left').select('df5.*',\n",
    "'df6.client_offlimit_linkedin_user_names')\\\n",
    ".drop('job_id_tmp')\\\n",
    ".fillna(value=\"No_offlimits\", subset=['client_offlimit_linkedin_user_names'])\\\n",
    ".fillna(value=\"Missing\").persist()\n",
    "# datasource_jobs3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "datasource_jobs4 = datasource_jobs3.where((F.col('job_status').isin('SETUP', 'FIRST_CICLE', 'SECOND_CICLE')) | (F.col('job_id')==2703))\\\n",
    ".where(F.col('job_id').isin(2689, 2703))\\\n",
    ".persist()\n",
    "# .where(F.col('job_id').isin(2689, 2546, 2666, 2214, 2578, 2345, 2303, 2454, 2222, 2645, 2651, 2665, 2671, 2669, 2449, 2675, 2267, 2703)).persist()\n",
    "datasource_jobs4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156460"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## Applications\n",
    "# datasource_applications = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"applications\", transformation_ctx = \"datasource_applications\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_applications2 = datasource_applications.withColumn(\"application_rank\", F.row_number().over(Window.partitionBy('linkedin_user_name').orderBy(F.col(\"application_creation_date_ts\").desc(), F.col(\"application_id_backoffice\").desc())))\\\n",
    ".withColumn(\"previous_applications_qty\", F.row_number().over(Window.partitionBy('linkedin_user_name').orderBy(F.col(\"application_creation_date_ts\").asc(), F.col(\"application_id_backoffice\").asc())))\\\n",
    ".withColumn(\"current_date\",F.current_date())\\\n",
    ".withColumn('days_since_last_application', ((F.datediff(F.col('current_date'), F.col('application_creation_date_ts')))))\\\n",
    ".where(F.col('application_rank')==1)\\\n",
    ".drop('current_date', 'application_rank')\n",
    "datasource_applications2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.64 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156460"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "datasource_applications3 = datasource_applications2.alias('df1')\\\n",
    ".join(datasource_jobs3.alias('df2'),\n",
    "on = datasource_applications2['job_id'] == datasource_jobs3['job_id'],\n",
    "how = 'left').select('df1.*',\n",
    "'df2.job_area',\n",
    "'df2.job_department',\n",
    "'df2.job_seniority',\n",
    "'df2.max_salary_offered',\n",
    "'df2.job_position',\n",
    "'df2.job_hiring_type',\n",
    "'df2.job_company_classification',\n",
    "'df2.all_job_skill_names')\\\n",
    ".withColumnRenamed('job_id', 'prev_application_job_id')\\\n",
    ".withColumnRenamed('job_area', 'prev_application_job_area')\\\n",
    ".withColumnRenamed('job_seniority', 'prev_application_job_seniority')\\\n",
    ".withColumnRenamed('max_salary_offered', 'prev_application_max_salary_offered')\\\n",
    ".withColumnRenamed('job_position', 'prev_application_job_position')\\\n",
    ".withColumnRenamed('job_hiring_type', 'prev_application_job_hiring_type')\\\n",
    ".withColumnRenamed('job_department', 'prev_application_job_department')\\\n",
    ".withColumnRenamed('job_company_classification', 'prev_application_job_company_classification')\\\n",
    ".withColumnRenamed('all_job_skill_names', 'prev_application_all_job_skill_names')\\\n",
    ".withColumnRenamed('linkedin_user_name', 'linkedin_user_name_tmp')\\\n",
    ".fillna(value=\"Missing\")\n",
    "datasource_applications3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "172070"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## Approaches\n",
    "# datasource_approaches = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"approaches\", transformation_ctx = \"datasource_approaches\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_approaches2 = datasource_approaches.withColumn(\"approach_rank\", F.row_number().over(Window.partitionBy('linkedin_user_name').orderBy(F.col(\"created_at_date_ts\").desc(), F.col(\"approach_id\").desc())))\\\n",
    ".withColumn(\"previous_approaches_qty\", F.row_number().over(Window.partitionBy('linkedin_user_name').orderBy(F.col(\"created_at_date_ts\").asc(), F.col(\"approach_id\").asc())))\\\n",
    ".withColumn(\"current_date\",F.current_date())\\\n",
    ".withColumn('days_since_last_approach', ((F.datediff(F.col('current_date'), F.col('created_at_date_ts')))))\\\n",
    ".where(F.col('approach_rank')==1)\\\n",
    ".drop('current_date', 'approach_rank')\n",
    "datasource_approaches2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "479750"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# datasource_approaches_status = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"approaches_status\", transformation_ctx = \"datasource_approaches_status\", additional_options={\"mergeSchema\": \"true\"}).toDF()\\\n",
    "# .orderBy(['approach_id', 'approach_status_date_ts'], ascending=False)\\\n",
    "# .dropDuplicates(subset = ['approach_id'])\n",
    "\n",
    "datasource_approaches_status = datasource_approaches_status0.orderBy(['approach_id', 'approach_status_date_ts'], ascending=False)\\\n",
    ".dropDuplicates(subset = ['approach_id'])\n",
    "datasource_approaches_status.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "172070"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "datasource_approaches3 = datasource_approaches2.alias('df1')\\\n",
    ".join(datasource_approaches_status.alias('df2'),\n",
    "on = datasource_approaches2['approach_id'] == datasource_approaches_status['approach_id'],\n",
    "how = 'left').select('df1.*', 'df2.approach_status')\n",
    "datasource_approaches3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "172070"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "datasource_approaches4 = datasource_approaches3.alias('df1')\\\n",
    ".join(datasource_jobs3.alias('df2'),\n",
    "on = datasource_approaches3['job_id'] == datasource_jobs3['job_id'],\n",
    "how = 'left').select('df1.*',\n",
    "'df2.job_area',\n",
    "'df2.job_department',\n",
    "'df2.job_seniority',\n",
    "'df2.max_salary_offered',\n",
    "'df2.job_position',\n",
    "'df2.job_hiring_type',\n",
    "'df2.job_company_classification',\n",
    "'df2.all_job_skill_names')\\\n",
    ".withColumnRenamed('job_id', 'prev_approach_job_id')\\\n",
    ".withColumnRenamed('approach_status', 'prev_approach_status')\\\n",
    ".withColumnRenamed('job_seniority', 'prev_approach_job_seniority')\\\n",
    ".withColumnRenamed('job_position', 'prev_approach_job_position')\\\n",
    ".withColumnRenamed('max_salary_offered', 'prev_approach_max_salary_offered')\\\n",
    ".withColumnRenamed('job_area', 'prev_approach_job_area')\\\n",
    ".withColumnRenamed('job_department', 'prev_approach_job_department')\\\n",
    ".withColumnRenamed('job_company_classification', 'prev_approach_job_company_classification')\\\n",
    ".withColumnRenamed('all_job_skill_names', 'prev_approach_all_job_skill_names')\\\n",
    ".withColumnRenamed('linkedin_user_name', 'linkedin_user_name_tmp')\\\n",
    ".fillna(value=\"Missing\")\n",
    "datasource_approaches4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 639 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9305"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## Prospects\n",
    "# datasource_prospects = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"prospects\", transformation_ctx = \"datasource_prospects\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_prospects2 = datasource_prospects\\\n",
    ".withColumn('prospect_location_country_tmp1', F.reverse(F.split(F.col('prospect_location'), ',')).getItem(0))\\\n",
    ".withColumn('prospect_location_country_tmp2', F.trim(F.regexp_replace(F.col('prospect_location_country_tmp1'), \" e Região\", \"\")))\\\n",
    ".withColumn('prospect_location_country', prospect_country_func(F.col('prospect_location_country_tmp2')))\\\n",
    ".withColumn('prospect_location_state_tmp1', F.reverse(F.split(F.col('prospect_location'), ',')).getItem(1))\\\n",
    ".withColumn('prospect_location_state_tmp2', F.trim(F.regexp_replace(F.col('prospect_location_state_tmp1'), \" e Região\", \"\")))\\\n",
    ".withColumn('prospect_location_state', prospect_region_international_func(F.col('prospect_location_state_tmp2'), F.col('prospect_location_country')))\\\n",
    ".withColumn('prospect_location_city_tmp1', F.reverse(F.split(F.col('prospect_location'), ',')).getItem(2))\\\n",
    ".withColumn('prospect_location_city_tmp2', F.trim(F.regexp_replace(F.col('prospect_location_city_tmp1'), \" e Região\", \"\")))\\\n",
    ".withColumn('prospect_location_city', prospect_region_international_func(F.col('prospect_location_city_tmp2'), F.col('prospect_location_country')))\\\n",
    ".withColumn('prospect_location_region_tmp1', prospect_region_func(F.col('prospect_location_state')))\\\n",
    ".withColumn('prospect_location_region', prospect_region_international_func(F.col('prospect_location_region_tmp1'), F.col('prospect_location_country')))\\\n",
    ".withColumn(\"prospect_department\", F.trim(F.split(F.col('prospect_area'),' - ').getItem(1)))\\\n",
    ".withColumn(\"prospect_area\", F.trim(F.split(F.col('prospect_area'),' ').getItem(0)))\\\n",
    ".withColumnRenamed('linkedin_user_name', 'linkedin_user_name_tmp')\\\n",
    ".withColumnRenamed('updated_at_date_ts', 'prospects_updated_at_date_ts')\\\n",
    ".fillna(value=\"Missing\")\n",
    "datasource_prospects2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasource_prospects2.select('prospect_location', 'prospect_location_city', 'prospect_location_state', 'prospect_location_region', 'prospect_location_country').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7371"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# datasource_prospects_smart_skills = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"prospects_smart_skills\", transformation_ctx = \"datasource_prospects_smart_skills\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_prospects_smart_skills2 = datasource_prospects_smart_skills.groupBy('linkedin_user_name').agg(F.count(F.col('prospect_smart_skill')).alias('prospect_smart_skills_qty'), F.collect_list(F.col('prospect_smart_skill')).alias('all_prospect_smart_skills'))\n",
    "\n",
    "datasource_prospects_smart_skills3 = datasource_prospects_smart_skills2.withColumn('all_prospect_smart_skills_word_count', word_count_func(F.col('all_prospect_smart_skills')))\\\n",
    ".withColumn('all_prospect_smart_skills', F.concat_ws(\",\", F.col('all_prospect_smart_skills')))\\\n",
    ".withColumnRenamed('linkedin_user_name', 'linkedin_user_name_tmp')\n",
    "datasource_prospects_smart_skills3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2103"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# datasource_prospects_diversity = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"prospects_diversity\", transformation_ctx = \"datasource_prospects_diversity\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_prospects_diversity2 = datasource_prospects_diversity.groupBy('linkedin_user_name').agg(F.count(F.col('prospect_diversity')).alias('prospect_diversity_qty'), F.collect_list(F.col('prospect_diversity')).alias('all_prospect_diversity'))\n",
    "\n",
    "datasource_prospects_diversity3 = datasource_prospects_diversity2.withColumn('all_prospect_diversity', F.concat_ws(\",\", F.col('all_prospect_diversity')))\\\n",
    ".withColumnRenamed('linkedin_user_name', 'linkedin_user_name_tmp')\n",
    "datasource_prospects_diversity3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8537"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# datasource_prospects_experiences = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"prospects_experiences\", transformation_ctx = \"datasource_prospects_experiences\", additional_options={\"mergeSchema\": \"true\"}).toDF()\n",
    "\n",
    "datasource_prospects_experiences2 = datasource_prospects_experiences\\\n",
    ".withColumn('experience_duration_months_clean', F.when(F.col('flag_last_experience')==True, None).otherwise(F.col('experience_duration_months')))\\\n",
    ".withColumn('last_company_classification_tmp', F.when(F.col('flag_last_experience')==True, F.col('company_classification')).otherwise(None))\\\n",
    ".withColumn('linkedin_company_user_name_tmp', F.when(F.col('flag_last_experience')==True, F.col('linkedin_company_user_name')).otherwise(None))\\\n",
    ".withColumn('last_experience_duration_months', F.when(F.col('flag_last_experience')==True, F.col('experience_duration_months')).otherwise(F.lit(0)))\\\n",
    ".withColumn('experience_descriptions_word_count', experience_word_count_func(F.col('experience_description')))\\\n",
    ".withColumn('last_experience_descriptions_word_count', F.when(F.col('flag_last_experience')==True, F.col('experience_descriptions_word_count')).otherwise(F.lit(0)))\\\n",
    ".groupBy('linkedin_user_name').agg(F.count(F.col('linkedin_company_name')).alias('prospect_experiences_qty'),\n",
    "                                   F.countDistinct(F.col('linkedin_company_name')).alias('prospect_companies_qty'),\n",
    "                                   F.sum(F.col('experience_duration_months')).alias('total_experience_months'),\n",
    "                                   F.min(F.col('experience_duration_months')).alias('experience_duration_months_min'),\n",
    "                                   F.sum(F.col('last_experience_duration_months')).alias('last_experience_duration_months'),\n",
    "                                   F.sum(F.col('last_experience_descriptions_word_count')).alias('last_experience_descriptions_word_count'),\n",
    "                                   F.avg(F.col('experience_duration_months_clean')).alias('experience_duration_months_clean_avg'),\n",
    "                                   F.collect_list(F.col('company_classification')).alias('all_company_classifications'),\n",
    "                                   F.collect_list(F.col('last_company_classification_tmp')).alias('last_company_classification_tmp2'),\n",
    "                                   F.collect_list(F.col('linkedin_company_user_name_tmp')).alias('linkedin_company_user_name_tmp2')\n",
    "                                  )\\\n",
    ".withColumn('last_company_classification', F.trim(F.reverse(F.col('last_company_classification_tmp2')).getItem(0)))\\\n",
    ".withColumn('linkedin_company_user_name', F.trim(F.reverse(F.col('linkedin_company_user_name_tmp2')).getItem(0)))\\\n",
    ".withColumn('all_company_classifications_word_count', company_word_count_func(F.col('all_company_classifications')))\\\n",
    ".withColumn('all_company_classifications_count', company_count_func(F.col('all_company_classifications')))\\\n",
    ".drop('all_company_classifications', 'last_company_classification_tmp', 'last_company_classification_tmp2', 'linkedin_company_user_name_tmp', 'linkedin_company_user_name_tmp2')\\\n",
    ".withColumnRenamed('linkedin_user_name', 'linkedin_user_name_tmp')\\\n",
    ".fillna(value=\"Missing\")\n",
    "datasource_prospects_experiences2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clients\n",
    "# datasource_clients = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"clients\", transformation_ctx = \"datasource_clients\", additional_options={\"mergeSchema\": \"true\"}).toDF().where(F.col('flag_off_limit')==True).dropDuplicates(subset=['linkedin_company_user_name'])\n",
    "datasource_clients = datasource_clients0.where(F.col('flag_off_limit')==True).dropDuplicates(subset=['linkedin_company_user_name'])\n",
    "datasource_clients.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "##Talents\n",
    "# datasource_talents = glueContext.create_dynamic_frame.from_catalog(database = \"prod-lakehouse-mirror\", table_name = \"talents\", transformation_ctx = \"datasource_talents\", additional_options={\"mergeSchema\": \"true\"}).toDF()\\\n",
    "# .where(F.col('talent_status') == 'AVAILABLE')\n",
    "\n",
    "datasource_talents2 = datasource_talents.alias('df1')\\\n",
    ".join(datasource_applications3.alias('df2'),\n",
    "on = datasource_talents['linkedin_user_name'] == datasource_applications3['linkedin_user_name_tmp'],\n",
    "how = 'left').select('df1.*',\n",
    "'df2.talent_last_email',\n",
    "'df2.application_current_phase_active',\n",
    "'df2.application_status',\n",
    "'df2.application_hiring_model',\n",
    "'df2.reason_disapproval',\n",
    "'df2.expected_salary_interval',\n",
    "'df2.max_salary_expected',\n",
    "'df2.previous_applications_qty',\n",
    "'df2.days_since_last_application',\n",
    "'df2.prev_application_job_area',\n",
    "'df2.prev_application_job_id',\n",
    "'df2.prev_application_job_department',\n",
    "'df2.prev_application_job_company_classification',\n",
    "'df2.prev_application_job_seniority',\n",
    "'df2.prev_application_max_salary_offered',\n",
    "'df2.prev_application_job_position',\n",
    "'df2.prev_application_all_job_skill_names',\n",
    "'df2.prev_application_job_hiring_type',\n",
    "'df2.pipefy_last_card_id_link').alias('df3')\\\n",
    ".join(datasource_approaches4.alias('df4'),\n",
    "on = datasource_talents['linkedin_user_name'] == datasource_approaches4['linkedin_user_name_tmp'],\n",
    "how = 'left').select('df3.*',\n",
    "'df4.prev_approach_job_id',\n",
    "'df4.prev_approach_job_area',\n",
    "'df4.prev_approach_status',\n",
    "'df4.previous_approaches_qty',\n",
    "'df4.prev_approach_job_company_classification',\n",
    "'df4.prev_approach_job_seniority',\n",
    "'df4.prev_approach_job_position',\n",
    "'df4.prev_approach_job_department',                     \n",
    "'df4.prev_approach_max_salary_offered',\n",
    "'df4.prev_approach_all_job_skill_names',\n",
    "'df4.days_since_last_approach',\n",
    "'df4.approach_id').alias('df5')\\\n",
    ".join(datasource_prospects2.alias('df6'),\n",
    "on = datasource_talents['linkedin_user_name'] == datasource_prospects2['linkedin_user_name_tmp'],\n",
    "how = 'left').select('df5.*',\n",
    "'df6.prospect_location_city',\n",
    "'df6.prospect_location_state',\n",
    "'df6.prospect_location_region',\n",
    "'df6.prospects_updated_at_date_ts',\n",
    "'df6.prospect_seniority',\n",
    "'df6.prospect_area',\n",
    "'df6.prospect_department',\n",
    "'df6.declared_seniority').alias('df7')\\\n",
    ".join(datasource_prospects_smart_skills3.alias('df8'),\n",
    "on = datasource_talents['linkedin_user_name'] == datasource_prospects_smart_skills3['linkedin_user_name_tmp'],\n",
    "how = 'left').select('df7.*',\n",
    "'df8.prospect_smart_skills_qty',\n",
    "'df8.all_prospect_smart_skills_word_count',\n",
    "'df8.all_prospect_smart_skills').alias('df9')\\\n",
    ".join(datasource_prospects_diversity3.alias('df10'),\n",
    "on = datasource_talents['linkedin_user_name'] == datasource_prospects_diversity3['linkedin_user_name_tmp'],\n",
    "how = 'left').select('df9.*',\n",
    "'df10.prospect_diversity_qty',\n",
    "'df10.all_prospect_diversity').alias('df11')\\\n",
    ".join(datasource_prospects_experiences2.alias('df12'),\n",
    "on = datasource_talents['linkedin_user_name'] == datasource_prospects_experiences2['linkedin_user_name_tmp'],\n",
    "how = 'left').select('df11.*',\n",
    "'df12.prospect_experiences_qty',\n",
    "'df12.prospect_companies_qty',\n",
    "'df12.total_experience_months',\n",
    "'df12.experience_duration_months_min',\n",
    "'df12.last_experience_duration_months',\n",
    "'df12.last_experience_descriptions_word_count',\n",
    "'df12.experience_duration_months_clean_avg',\n",
    "'df12.last_company_classification',\n",
    "'df12.all_company_classifications_word_count',\n",
    "'df12.all_company_classifications_count',\n",
    "'df12.linkedin_company_user_name')\\\n",
    ".withColumn('talent_info_source', talent_info_source_func(F.col('previous_approaches_qty'), F.col('previous_applications_qty'), F.col('prospects_updated_at_date_ts')))\\\n",
    ".withColumn('linkedin', F.concat(F.lit('https://www.linkedin.com/in/'), F.trim(F.col('linkedin_user_name'))))\\\n",
    ".drop('linkedin_user_name_tmp')\\\n",
    ".fillna(value=\"Missing\")\n",
    "\n",
    "datasource_talents3 = datasource_talents2.alias('df1')\\\n",
    ".join(datasource_clients.alias('df2'),\n",
    "on = datasource_talents2['linkedin_company_user_name'] == datasource_clients['linkedin_company_user_name'],\n",
    "how = 'left').select('df1.*',\n",
    "'df2.flag_off_limit')\\\n",
    ".where(F.col('flag_off_limit').isNull())\n",
    "\n",
    "datasource_talents3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasource_talents3.groupBy('all_prospect_diversity').count().show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## Cross join\n",
    "additional_features=['linkedin_user_name'\n",
    ",'talent_status'\n",
    ",'job_id'\n",
    ",'prospect_area'\n",
    ",'prospect_department'\n",
    ",'prev_approach_job_department'\n",
    ",'job_department'\n",
    ",'all_job_skill_names'\n",
    ",'prev_approach_all_job_skill_names'\n",
    ",'all_prospect_smart_skills'\n",
    ",'prev_application_all_job_skill_names'\n",
    ",'talent_previous_job_id'\n",
    ",'talent_area'\n",
    ",'talent_department'\n",
    ",'talent_skills'\n",
    ",'talent_expected_salary'\n",
    ",'talent_expected_salary_interval'\n",
    ",'talent_hiring_model'\n",
    ",'talent_location_state'\n",
    ",'talent_location_city'\n",
    ",'talent_seniority'\n",
    ",'linkedin'\n",
    ",'pipefy_last_card_id_link'\n",
    ",'talent_diversities'\n",
    ",'talent_diversity_flag'\n",
    ",'min_salary_offered'\n",
    ",'job_position'\n",
    ",'talent_position'\n",
    ",'talent_last_email'\n",
    "]\n",
    "all_features = selected_features + additional_features\n",
    "\n",
    "cross_join_talents_jobs = datasource_talents3.crossJoin(datasource_jobs4)\\\n",
    ".withColumn(\"company_classification_migration\", F.trim(F.concat(F.col('last_company_classification'), F.lit('-to-'), F.col('job_company_classification'))))\\\n",
    ".withColumn(\"prev_approach_job_area_to_job_area\", F.trim(F.concat(F.col('prev_approach_job_area'), F.lit('-to-'), F.col('job_area'))))\\\n",
    ".withColumn(\"prev_application_job_area_to_job_area\", F.trim(F.concat(F.col('prev_application_job_area'), F.lit('-to-'), F.col('job_area'))))\\\n",
    ".withColumn(\"prev_approach_company_classification_migration\", F.trim(F.concat(F.col('prev_approach_job_company_classification'), F.lit('-to-'), F.col('job_company_classification'))))\\\n",
    ".withColumn(\"prev_application_company_classification_migration\", F.trim(F.concat(F.col('prev_application_job_company_classification'), F.lit('-to-'), F.col('job_company_classification'))))\\\n",
    ".withColumn(\"prev_approach_job_seniority_to_job_seniority\", F.trim(F.concat(F.col('prev_approach_job_seniority'), F.lit('-to-'), F.col('job_seniority'))))\\\n",
    ".withColumn(\"prev_application_job_seniority_to_job_seniority\", F.trim(F.concat(F.col('prev_application_job_seniority'), F.lit('-to-'), F.col('job_seniority'))))\\\n",
    ".withColumn(\"prev_approach_job_position_to_job_position\", F.trim(F.concat(F.col('prev_approach_job_position'), F.lit('-to-'), F.col('job_position'))))\\\n",
    ".withColumn(\"prev_application_job_position_to_job_position\", F.trim(F.concat(F.col('prev_application_job_position'), F.lit('-to-'), F.col('job_position'))))\\\n",
    ".withColumn(\"prev_application_job_hiring_type_to_job_hiring_type\", F.trim(F.concat(F.col('prev_application_job_hiring_type'), F.lit('-to-'), F.col('job_hiring_type'))))\\\n",
    ".withColumn(\"prospect_seniority_migration\", F.trim(F.concat(F.col('prospect_seniority'), F.lit('-to-'), F.col('job_seniority'))))\\\n",
    ".withColumn(\"declared_seniority_migration\", F.trim(F.concat(F.col('declared_seniority'), F.lit('-to-'), F.col('job_seniority'))))\\\n",
    ".withColumn(\"prospect_area_migration\", F.trim(F.concat(F.col('prospect_area'), F.lit('-to-'), F.col('job_area'))))\\\n",
    ".withColumn(\"max_salary_offered_to_prev_approach\", (F.col('max_salary_offered') - F.col('prev_approach_max_salary_offered')))\\\n",
    ".withColumn(\"max_salary_offered_to_prev_application\", (F.col('max_salary_offered') - F.col('prev_application_max_salary_offered')))\\\n",
    ".withColumn('prev_approach_max_salary_offered', F.col('prev_approach_max_salary_offered').cast(FloatType()))\\\n",
    ".fillna(value=-1, subset=[\"prev_approach_max_salary_offered\"])\\\n",
    ".withColumn('prev_application_max_salary_offered', F.col('prev_application_max_salary_offered').cast(FloatType()))\\\n",
    ".fillna(value=21000.0, subset=[\"prev_application_max_salary_offered\"])\\\n",
    ".withColumn('max_salary_expected', F.col('max_salary_expected').cast(FloatType()))\\\n",
    ".fillna(value=-1, subset=[\"max_salary_expected\"])\\\n",
    ".withColumn('min_salary_offered', F.col('min_salary_offered').cast(FloatType()))\\\n",
    ".fillna(value=0, subset=[\"min_salary_offered\"])\\\n",
    ".withColumn('max_salary_offered', F.col('max_salary_offered').cast(FloatType()))\\\n",
    ".fillna(value=-1, subset=[\"max_salary_offered\"])\\\n",
    ".withColumn('previous_applications_qty', F.col('previous_applications_qty').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"previous_applications_qty\"])\\\n",
    ".withColumn('previous_approaches_qty', F.col('previous_approaches_qty').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"previous_approaches_qty\"])\\\n",
    ".withColumn('days_since_last_application', F.col('days_since_last_application').cast(IntegerType()))\\\n",
    ".fillna(value=999, subset=[\"days_since_last_application\"])\\\n",
    ".withColumn('days_since_last_approach', F.col('days_since_last_approach').cast(IntegerType()))\\\n",
    ".fillna(value=999, subset=[\"days_since_last_approach\"])\\\n",
    ".withColumn('prospect_smart_skills_qty', F.col('prospect_smart_skills_qty').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"prospect_smart_skills_qty\"])\\\n",
    ".withColumn('prospect_experiences_qty', F.col('prospect_experiences_qty').cast(IntegerType()))\\\n",
    ".fillna(value=6, subset=[\"prospect_experiences_qty\"])\\\n",
    ".withColumn('prospect_companies_qty', F.col('prospect_companies_qty').cast(IntegerType()))\\\n",
    ".fillna(value=5, subset=[\"prospect_companies_qty\"])\\\n",
    ".withColumn('total_experience_months', F.col('total_experience_months').cast(IntegerType()))\\\n",
    ".fillna(value=111, subset=[\"total_experience_months\"])\\\n",
    ".withColumn('experience_duration_months_min', F.col('experience_duration_months_min').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"experience_duration_months_min\"])\\\n",
    ".withColumn('last_experience_duration_months', F.col('last_experience_duration_months').cast(IntegerType()))\\\n",
    ".fillna(value=41, subset=[\"last_experience_duration_months\"])\\\n",
    ".withColumn('max_salary_offered_to_prev_application', F.col('max_salary_offered_to_prev_application').cast(FloatType()))\\\n",
    ".fillna(value=11902.34, subset=[\"max_salary_offered_to_prev_application\"])\\\n",
    ".withColumn('max_salary_offered_to_prev_approach', F.col('max_salary_offered_to_prev_approach').cast(FloatType()))\\\n",
    ".fillna(value=11390.0, subset=[\"max_salary_offered_to_prev_approach\"])\\\n",
    ".withColumn('all_job_question_correct_answers_word_count', F.col('all_job_question_correct_answers_word_count').cast(IntegerType()))\\\n",
    ".fillna(value=1, subset=[\"all_job_question_correct_answers_word_count\"])\\\n",
    ".withColumn('all_job_skill_names_word_count', F.col('all_job_skill_names_word_count').cast(IntegerType()))\\\n",
    ".fillna(value=1, subset=[\"all_job_skill_names_word_count\"])\\\n",
    ".withColumn('experience_duration_months_clean_avg', F.col('experience_duration_months_clean_avg').cast(FloatType()))\\\n",
    ".fillna(value=399, subset=[\"experience_duration_months_clean_avg\"])\\\n",
    ".withColumn('all_company_classifications_count', F.col('all_company_classifications_count').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"all_company_classifications_count\"])\\\n",
    ".withColumn('last_experience_descriptions_word_count', F.col('last_experience_descriptions_word_count').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"last_experience_descriptions_word_count\"])\\\n",
    ".withColumn('all_prospect_smart_skills_word_count', F.col('all_prospect_smart_skills_word_count').cast(IntegerType()))\\\n",
    ".fillna(value=1, subset=[\"all_prospect_smart_skills_word_count\"])\\\n",
    ".withColumn('all_company_classifications_word_count', F.col('all_company_classifications_word_count').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"all_company_classifications_word_count\"])\\\n",
    ".fillna(value=0, subset=[\"prev_application_job_id\"])\\\n",
    ".fillna(value=0, subset=[\"prev_approach_job_id\"])\\\n",
    ".withColumn('talent_area', F.coalesce((F.when(F.col('prev_application_job_area')=='Missing', None).otherwise(F.col('prev_application_job_area'))), (F.when(F.col('prev_approach_job_area')=='Missing', None).otherwise(F.col('prev_approach_job_area'))), (F.when(F.col('prospect_area')=='Missing', None).otherwise(F.col('prospect_area'))), F.lit('Missing')))\\\n",
    ".withColumn('talent_department', F.coalesce((F.when(F.col('prev_application_job_department')=='Missing', None).otherwise(F.col('prev_application_job_department'))), (F.when(F.col('prev_approach_job_department')=='Missing', None).otherwise(F.col('prev_approach_job_department'))), (F.when(F.col('prospect_department')=='Missing', None).otherwise(F.col('prospect_department')))))\\\n",
    ".withColumn('talent_skills', F.coalesce((F.when(F.col('all_prospect_smart_skills')=='Missing', None).otherwise(F.col('all_prospect_smart_skills'))), (F.when(F.col('prev_application_all_job_skill_names')=='Missing', None).otherwise(F.col('prev_application_all_job_skill_names'))), (F.when(F.col('prev_approach_all_job_skill_names')=='Missing', None).otherwise(F.col('prev_approach_all_job_skill_names'))), F.lit('Missing')))\\\n",
    ".withColumn('talent_seniority', F.coalesce((F.when(F.col('prev_application_job_seniority')=='Missing', None).otherwise(F.col('prev_application_job_seniority'))), (F.when(F.col('prev_approach_job_seniority')=='Missing', None).otherwise(F.col('prev_approach_job_seniority'))), (F.when(F.col('prospect_seniority')=='Missing', None).otherwise(F.col('prospect_seniority'))), (F.when(F.col('declared_seniority')=='Missing', None).otherwise(F.col('declared_seniority')))))\\\n",
    ".withColumn('talent_position', F.coalesce((F.when(F.col('prev_application_job_position')=='Missing', None).otherwise(F.col('prev_application_job_position'))), (F.when(F.col('prev_approach_job_position')=='Missing', None).otherwise(F.col('prev_approach_job_position'))), F.lit('Missing')))\\\n",
    ".withColumn('talent_expected_salary', F.col('max_salary_expected'))\\\n",
    ".withColumn('talent_expected_salary_interval', F.when(F.col('prev_application_job_seniority')=='Missing', F.lit('u) Missing')).otherwise(F.col('expected_salary_interval')))\\\n",
    ".withColumn('talent_hiring_model', F.col('application_hiring_model'))\\\n",
    ".withColumn('talent_location_state', F.col('prospect_location_state'))\\\n",
    ".withColumn('talent_location_city', F.col('prospect_location_city'))\\\n",
    ".withColumn('talent_previous_job_id', F.col('prev_application_job_id'))\\\n",
    ".withColumn('talent_previous_job_id', F.col('prev_application_job_id'))\\\n",
    ".withColumn('match_skill_count', match_skill_count_func(F.col('all_job_skill_names'), F.col('talent_skills')))\\\n",
    ".withColumn('match_skill_count', F.col('match_skill_count').cast(IntegerType()))\\\n",
    ".fillna(value=0, subset=[\"match_skill_count\"])\\\n",
    ".withColumnRenamed('all_prospect_diversity', 'talent_diversities')\\\n",
    ".withColumn('talent_diversity_flag', F.when(F.col('talent_diversities')=='Missing', False).otherwise(True))\\\n",
    ".where((F.col('linkedin_company_user_name').isin(F.col('client_offlimit_linkedin_user_names'))==False)\n",
    "       & (F.col('job_id') != F.col('prev_application_job_id')) \n",
    "       & (F.col('job_id') != F.col('prev_approach_job_id'))\n",
    "       & ((F.col('job_area') == F.col('talent_area')) | (F.col('talent_area') == 'Missing')))\\\n",
    ".fillna(value=\"Missing\")\\\n",
    ".select(all_features)\n",
    "cross_join_talents_jobs.count()\n",
    "       \n",
    "# .where((F.col('job_status').isin('SETUP', 'FIRST_CICLE', 'SECOND_CICLE')) | (F.col('job_id').isin(2513, 2627)))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_join_talents_jobs.select('job_id', 'linkedin_user_name','prev_application_job_id', 'prev_approach_job_id', 'linkedin_company_user_name', 'client_offlimit_linkedin_user_names').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_join_talents_jobs.select('job_id', 'linkedin_user_name','prev_application_job_id', 'prev_approach_job_id', 'linkedin_company_user_name', 'client_offlimit_linkedin_user_names').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Escorar a base, criar o score e o rating\n",
    "settings = H2OMOJOSettings(convertUnknownCategoricalLevelsToNa = True, convertInvalidNumbersToNa = True)\n",
    "model = H2OMOJOModel.createFromMojo(PathModelMojo, settings)\n",
    "\n",
    "sherlock_smart_list = model.transform(cross_join_talents_jobs)\n",
    "sherlock_smart_list.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sherlock_smart_list.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sherlock_smart_list2 = sherlock_smart_list.withColumn('predict', F.col('detailed_prediction.label').cast(IntegerType()))\\\n",
    ".withColumn('probability', F.col('detailed_prediction.probabilities.1'))\\\n",
    ".withColumn('rating', ratings_func((F.col('probability'))))\\\n",
    ".withColumn('suggestion', suggestion_func(F.col('predict')))\\\n",
    ".withColumn('order_area', F.when((F.col('job_area') == F.col('talent_area')), F.lit(0)).otherwise(F.lit(1)))\\\n",
    ".withColumn('order_department', F.when((F.col('job_department') == F.col('talent_department')), F.lit(0)).otherwise(F.lit(1)))\\\n",
    ".withColumn('order_position', F.when((F.col('job_position') == F.col('talent_position')), F.lit(0)).otherwise(F.lit(1)))\\\n",
    ".withColumn('order_salary', F.when(((F.col('talent_expected_salary') >= F.col('min_salary_offered')) & (F.col('talent_expected_salary') <= F.col('max_salary_offered'))), F.lit(0)).otherwise(F.lit(1)))\\\n",
    ".withColumn('talent_expected_salary', F.when(F.col('talent_expected_salary')==-1, F.lit(None)).otherwise(F.col('talent_expected_salary')))\\\n",
    ".drop('detailed_prediction', 'prediction', 'talent_status')\n",
    "# sherlock_smart_list2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sherlock_smart_list2.select('order_area', 'job_area', 'talent_area', 'order_department', 'job_department', 'talent_department', 'order_salary', 'talent_expected_salary', 'min_salary_offered', 'max_salary_offered', 'match_skill_count').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instr\n",
    "# levenshtein\n",
    "# locate\n",
    "# sherlock_smart_list2.withColumn(\"levenshtein\", F.levenshtein(F.col(\"all_job_skill_names\"), F.col(\"all_prospect_smart_skills\")))\\\n",
    "# .select('levenshtein', 'all_job_skill_names', 'all_prospect_smart_skills').show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3349.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 239.0 failed 1 times, most recent failure: Lost task 8.0 in stage 239.0 (TID 14313) (DESKTOP-BSGB07T executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3006)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3005)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3005)\r\n\tat sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \"\"\"\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3349.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 239.0 failed 1 times, most recent failure: Lost task 8.0 in stage 239.0 (TID 14313) (DESKTOP-BSGB07T executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3006)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3005)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3005)\r\n\tat sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sherlock_smart_list3 = sherlock_smart_list2.withColumn(\"talent_rank\", F.row_number().over(Window.partitionBy('job_id').orderBy(F.col(\"order_area\").asc(), F.col(\"predict\").desc(), F.col(\"order_department\").asc(), F.col(\"order_salary\").asc(), F.col(\"match_skill_count\").desc(), F.col(\"order_position\").asc(), F.col(\"probability\").desc())))\\\n",
    ".where(F.col('talent_rank')<=20000)\n",
    "\n",
    "first_cols = ['job_id', 'linkedin_user_name', 'probability', 'rating', 'linkedin', 'pipefy_last_card_id_link', 'talent_area', 'talent_department', 'talent_expected_salary', 'talent_hiring_model', 'talent_location_state', 'talent_seniority', 'talent_skills']\n",
    "other_cols = sorted([c for c in sherlock_smart_list3.columns if c not in first_cols])\n",
    "rearanged_cols = first_cols + other_cols\n",
    "sherlock_smart_list3 = sherlock_smart_list3.select(rearanged_cols)\n",
    "sherlock_smart_list3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Salvar no Lakehouse no format parquet\n",
    "# sherlock_smart_list3.write.mode(\"overwrite\").format(\"parquet\").partitionBy('suggestion').save(\"s3://prod-lakehouse-mirror/sherlock_smart_list\")\n",
    "\n",
    "sherlock_smart_list3.select('job_id', 'linkedin_user_name', 'probability', 'rating', 'talent_rank', 'suggestion', 'prospect_area_migration', 'talent_info_source', 'talent_department', 'talent_area', 'talent_previous_job_id', 'talent_skills'\n",
    ",'talent_expected_salary'\n",
    ",'talent_expected_salary_interval'\n",
    ",'talent_hiring_model'\n",
    ",'talent_location_state'\n",
    ",'talent_seniority'\n",
    ",'match_skill_count'\n",
    ",'linkedin'\n",
    ",'pipefy_last_card_id_link')\\\n",
    ".toPandas().to_csv('sherlock_smart_list3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ,'talent_previous_job_id'\n",
    "# ,'talent_area'\n",
    "# ,'talent_department'\n",
    "# ,'talent_skills'\n",
    "# ,'talent_expected_salary'\n",
    "# ,'talent_expected_salary_interval'\n",
    "# ,'talent_hiring_model'\n",
    "# ,'talent_location_state'\n",
    "# ,'talent_seniority'\n",
    "# ,'match_skill_count'\n",
    "# ,'linkedin'\n",
    "# ,'pipefy_last_card_id_link'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare local with AWS Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "job_id                             340000\n",
       "linkedin_user_name                 340000\n",
       "probability                        340000\n",
       "rating                             340000\n",
       "linkedin                           340000\n",
       "match_skill_count                  340000\n",
       "pipefy_last_card_id_link           340000\n",
       "prospect_area_migration            340000\n",
       "talent_area                        340000\n",
       "talent_department                  340000\n",
       "talent_expected_salary             340000\n",
       "talent_expected_salary_interval    340000\n",
       "talent_hiring_model                340000\n",
       "talent_info_source                 340000\n",
       "talent_location_state              340000\n",
       "talent_previous_job_id              65597\n",
       "talent_rank                        340000\n",
       "talent_seniority                   340000\n",
       "talent_skills                      340000\n",
       "suggestion                         340000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "select *\n",
    "from \"prod-lakehouse-mirror\".sherlock_smart_list\"\"\"\n",
    "sherlock_smart_list_local = wr.athena.read_sql_query(query, database=\"prod-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result')\n",
    "sherlock_smart_list_local.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "job_id                     300000\n",
       "linkedin_user_name         300000\n",
       "probability                300000\n",
       "rating                     300000\n",
       "linkedin                   300000\n",
       "                            ...  \n",
       "talent_previous_job_id     169774\n",
       "talent_rank                300000\n",
       "talent_status              300000\n",
       "total_experience_months    300000\n",
       "suggestion                 300000\n",
       "Length: 79, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "select *\n",
    "from \"dev-lakehouse-mirror\".sherlock_smart_list\"\"\"\n",
    "sherlock_smart_list_aws = wr.athena.read_sql_query(query, database=\"dev-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result')\n",
    "sherlock_smart_list_aws.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataComPy Comparison\n",
      "--------------------\n",
      "\n",
      "DataFrame Summary\n",
      "-----------------\n",
      "\n",
      "  DataFrame  Columns    Rows\n",
      "0     local       20  340000\n",
      "1       aws       79  300000\n",
      "\n",
      "Column Summary\n",
      "--------------\n",
      "\n",
      "Number of columns in common: 20\n",
      "Number of columns in local but not in aws: 0\n",
      "Number of columns in aws but not in local: 59\n",
      "\n",
      "Row Summary\n",
      "-----------\n",
      "\n",
      "Matched on: job_id, linkedin_user_name\n",
      "Any duplicates on match values: No\n",
      "Absolute Tolerance: 0\n",
      "Relative Tolerance: 0\n",
      "Number of rows in common: 44,053\n",
      "Number of rows in local but not in aws: 295,947\n",
      "Number of rows in aws but not in local: 255,947\n",
      "\n",
      "Number of rows with some compared columns unequal: 44,053\n",
      "Number of rows with all compared columns equal: 0\n",
      "\n",
      "Column Comparison\n",
      "-----------------\n",
      "\n",
      "Number of columns compared with some values unequal: 5\n",
      "Number of columns compared with all values equal: 15\n",
      "Total number of values which compare unequal: 110,388\n",
      "\n",
      "Columns with Unequal Values or Types\n",
      "------------------------------------\n",
      "\n",
      "          Column local dtype aws dtype  # Unequal      Max Diff  # Null Diff\n",
      "3    probability      object    object      28150      0.219896            0\n",
      "2         rating      object    object       3893      5.000000            0\n",
      "4     suggestion      object    object        456      0.000000            0\n",
      "1    talent_rank      object    object      44053  19596.000000            0\n",
      "0  talent_skills      object    object      33836      0.000000            0\n",
      "\n",
      "Sample Rows with Unequal Values\n",
      "-------------------------------\n",
      "\n",
      "       job_id            linkedin_user_name                                                                             talent_skills (local)                                                                               talent_skills (aws)\n",
      "61795    2671               wesleysilva-dev                           css,docker,git,html,jquery,laravel,linux-,mysql,php,redis,sql,wordpress                           git,redis,sql,jquery,wordpress,html,laravel,linux-,mysql,php,css,docker\n",
      "167902   2645                   rafaelruwer                                              android,c,git,iot,macos,mvc,mvvm,rxswift,scrum,swift                                              git,iot,android,macos,mvc,rxswift,c,mvvm,swift,scrum\n",
      "147808   2578       celiara-regina-58bb60b4                               access,business-intelligence,cognos,etl,excel,linux-,oracle,sql,vba                               oracle,business-intelligence,vba,sql,excel,etl,cognos,linux-,access\n",
      "321937   2222            cassio-de-medeiros                                                                   business-intelligence,logistics                                                                   logistics,business-intelligence\n",
      "29518    2671  luis-nunes-martins-39aabb103                                                                                  c,postgresql,wpf                                                                                  c,wpf,postgresql\n",
      "180572   2222                  vn-rodrigues         apache,apache-spark,aws,cloud,data-analysis,etl,python,ruby,ruby-on-rails,scala,spark,sql         spark,apache-spark,cloud,python,scala,aws,sql,apache,data-analysis,ruby,ruby-on-rails,etl\n",
      "110753   2666       melina-bravim-36609a104                                                                                     angular,vuejs                                                                                     vuejs,angular\n",
      "249715   2222                 claudiomoreno  aws,aws-glue,cognos,data-analyst,dynamodb,etl,oracle,oracle-pl-sql,perl,procurement,security,sql  perl,oracle-pl-sql,sql,security,aws,aws-glue,dynamodb,etl,cognos,data-analyst,oracle,procurement\n",
      "44339    2675        jeremias-lima-15039897                                                                         react,react-native,sketch                                                                         sketch,react,react-native\n",
      "138475   2578     henrique-pizarro-a9945641                                                                                         r,sas,sql                                                                                         sas,sql,r\n",
      "\n",
      "       job_id                  linkedin_user_name talent_rank (local) talent_rank (aws)\n",
      "259349   2345                     camila-barbieri                 275             12162\n",
      "13217    2449                    carlosaugustovaz                 930              7791\n",
      "9957     2222                     joaquinmenendez                2610             10048\n",
      "314941   2546                    bruno-brancalhão                1500             10527\n",
      "29384    2671              valter-godoy-33aa41125                1212             13159\n",
      "138402   2578                        amonteirodba                 812              3590\n",
      "17114    2669  guilherme-guisso-da-silva-50349392                 817             18245\n",
      "10107    2222                juliana-vaz-31881723                2760             11526\n",
      "115534   2454                            jpmicena                4632             18192\n",
      "15128    2214                   alextrindadecosta                 826              7277\n",
      "\n",
      "       job_id         linkedin_user_name rating (local) rating (aws)\n",
      "260632   2578    carla-casaroti-7120ba36              8            9\n",
      "210030   2345  evandro-oliveira-87210a51              3            5\n",
      "300170   2222              iago-ferranti              4            2\n",
      "252879   2671       kleber-melo-8b139b35              3            2\n",
      "336526   2578                 alex-souza              4            5\n",
      "13069    2449      filipe-cruz-developer              8            9\n",
      "168726   2214                alinedonato              1            2\n",
      "183415   2671      caio-maselli-76b53199              3            4\n",
      "72730    2578             dandaramcsousa              3            4\n",
      "14368    2214            lorrane-meneses              6            7\n",
      "\n",
      "       job_id              linkedin_user_name   probability (local)    probability (aws)\n",
      "12347    2449          gabriel-simon-gianotti    0.3871541053945665   0.3885057263452813\n",
      "138396   2578  carlos-filipe-almeida-a721692b   0.04406319351584734  0.04356173882351397\n",
      "259253   2345                    tecnoricardo   0.17592371525781747  0.25157175293904127\n",
      "250944   2454  guilherme-florentino-94906b146   0.06866240216173321  0.06704591843734696\n",
      "117323   2671      jonathan-matheus-a53844148   0.04532158406468462  0.07084938681500269\n",
      "259390   2345           carlos-alberto-gaspar    0.3764698469675097  0.49092975190064375\n",
      "12852    2449                     rafaelgripp   0.21724708623938446  0.21574460920364802\n",
      "167997   2645        marcos-contente-26857520  0.015794941242845355  0.01569411640727131\n",
      "226512   2214           marco-telles-55b1a031    0.1312513892672822  0.13823925195106232\n",
      "169784   2449                     beatriz1996   0.13110326699750993   0.1289490162096039\n",
      "\n",
      "       job_id                linkedin_user_name suggestion (local) suggestion (aws)\n",
      "209927   2345                   brunocascaolima           Repensar          Abordar\n",
      "15456    2449  evelyn-fernandes-rocha-851639157           Repensar          Abordar\n",
      "9749     2222                          marttosc            Abordar         Repensar\n",
      "61625    2671       lincoln-bergstron-920201157           Repensar          Abordar\n",
      "113953   2222               antoniojuniorvabreu           Repensar          Abordar\n",
      "28731    2651                      luizvbemidio           Repensar          Abordar\n",
      "114034   2222                      brunacallais           Repensar          Abordar\n",
      "209920   2345               guilherme-steixeira           Repensar          Abordar\n",
      "182091   2651                         georgewcf           Repensar          Abordar\n",
      "168429   2645        guilherme-israel-8aa357134           Repensar          Abordar\n",
      "\n",
      "Sample Rows Only in local (First 10 Columns)\n",
      "--------------------------------------------\n",
      "\n",
      "       job_id                               linkedin_user_name           probability rating                                                                     linkedin match_skill_count                     pipefy_last_card_id_link     prospect_area_migration  talent_area talent_department\n",
      "142694   2222                            victorfernandesmatias   0.08723497300493831      4                            https://www.linkedin.com/in/victorfernandesmatias                 0  https://app.pipefy.com/open-cards/440987557         Engineering-to-Data  Engineering          Back-end\n",
      "218975   2675                                    renatasalvino   0.06379660758597314      3                                    https://www.linkedin.com/in/renatasalvino                 0  https://app.pipefy.com/open-cards/501168123              Infra-to-Agile        Infra      Devops / SRE\n",
      "207975   2665                               ygor-sad-229748117   0.08682825928544886      4                               https://www.linkedin.com/in/ygor-sad-229748117                 0                                      Missing      Engineering-to-Product  Engineering          Back-end\n",
      "177085   2578                                       ramomfilho  0.036997688320386926      1                                       https://www.linkedin.com/in/ramomfilho                 1                                      Missing         Engineering-to-Data  Engineering          Back-end\n",
      "284632   2651                          henrique-lati-3095b4167  0.019920287610475918      1                          https://www.linkedin.com/in/henrique-lati-3095b4167                 0                                      Missing  Engineering-to-Engineering  Engineering          Back-end\n",
      "146243   2303  fernando-matheus-comandolli-fürbringer-b8059650   0.12236136801470668      5  https://www.linkedin.com/in/fernando-matheus-comandolli-fürbringer-b8059650                 0                                      Missing    Engineering-to-Segurança  Engineering          Back-end\n",
      "123481   2214         guilherme-barbosa-alves-de-lara-7a3070b9   0.02957519965697934      1         https://www.linkedin.com/in/guilherme-barbosa-alves-de-lara-7a3070b9                 0                                      Missing  Engineering-to-Engineering  Engineering          Back-end\n",
      "156586   2546                          paulo-gabriel-704162125   0.09261252639825017      5                          https://www.linkedin.com/in/paulo-gabriel-704162125                 1                                      Missing  Engineering-to-Arquitetura  Engineering          Back-end\n",
      "67528    2222                                       yansilva89   0.10206154943891027      5                                       https://www.linkedin.com/in/yansilva89                 0                                      Missing         Engineering-to-Data  Engineering         Front-end\n",
      "135429   2651                                       guidocunha   0.04411539807933522      2                                       https://www.linkedin.com/in/guidocunha                 0                                      Missing  Engineering-to-Engineering  Engineering        Full-stack\n",
      "\n",
      "Sample Rows Only in aws (First 10 Columns)\n",
      "------------------------------------------\n",
      "\n",
      "       job_id             linkedin_user_name          probability rating                                                   linkedin                     pipefy_last_card_id_link  talent_area      talent_department talent_expected_salary talent_hiring_model\n",
      "454245   2669                    jairo-olima   0.5294392004573006     10                    https://www.linkedin.com/in/jairo-olima  https://app.pipefy.com/open-cards/555562572        Infra                    SAP                 3000.0                 CLT\n",
      "441609   2578          fábio-mourão-31b17147   0.4357603953917737     10          https://www.linkedin.com/in/fábio-mourão-31b17147  https://app.pipefy.com/open-cards/545737829         Data         BI / Analytics                 2000.0                 CLT\n",
      "387451   2666    saulo-silva-filho-88241220a   0.1260847998707569      6    https://www.linkedin.com/in/saulo-silva-filho-88241220a  https://app.pipefy.com/open-cards/407313692  Engineering             Full-stack                 4500.0             Missing\n",
      "500419   2651                         renant  0.24440616932790615      9                         https://www.linkedin.com/in/renant  https://app.pipefy.com/open-cards/531655921  Engineering               Back-end                 7000.0                 CLT\n",
      "526496   2214                    rafaelmaeda   0.2797719256920808     10                    https://www.linkedin.com/in/rafaelmaeda  https://app.pipefy.com/open-cards/542308556  Engineering               Back-end                 4500.0                 CLT\n",
      "524812   2578      marciel-barbosa-289686221  0.13129047371705882      6      https://www.linkedin.com/in/marciel-barbosa-289686221  https://app.pipefy.com/open-cards/448690145         Data         BI / Analytics                 2000.0             Missing\n",
      "507309   2669         kátia-santana-2b553951  0.26054325842715326      9         https://www.linkedin.com/in/kátia-santana-2b553951  https://app.pipefy.com/open-cards/479704590        Infra  Sustentação / Suporte                 2500.0                 CLT\n",
      "544074   2651          andrea-simoes-herdade   0.6293246926481321     10          https://www.linkedin.com/in/andrea-simoes-herdade  https://app.pipefy.com/open-cards/563106370  Engineering                     QA                10500.0                 CLT\n",
      "525076   2303           luiz-bento-5a8b8815b   0.5226805780944366     10           https://www.linkedin.com/in/luiz-bento-5a8b8815b                                      Missing      Missing                Missing                   -1.0             Missing\n",
      "531440   2675  josé-leandro-santos-117858222   0.4209077904867035     10  https://www.linkedin.com/in/josé-leandro-santos-117858222  https://app.pipefy.com/open-cards/494060424        Infra  Sustentação / Suporte                 2500.0                 CLT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datacompy\n",
    "#comparacao das duas bases\n",
    "compare = datacompy.Compare(\n",
    "    sherlock_smart_list_local.astype(str),\n",
    "    sherlock_smart_list_aws.astype(str),\n",
    "    join_columns=['job_id', 'linkedin_user_name'],  #You can also specify a list of columns\n",
    "    abs_tol=0, #Optional, defaults to 0\n",
    "    rel_tol=0, #Optional, defaults to 0\n",
    "    df1_name='local', #Optional, defaults to 'df1'\n",
    "    df2_name='aws' #Optional, defaults to 'df2'\n",
    "    )\n",
    "compare.matches(ignore_extra_columns=False)\n",
    "# False\n",
    "\n",
    "# This method prints out a human-readable report summarizing and sampling differences\n",
    "print(compare.report())\n",
    "\n",
    "only_df1=compare.df1_unq_rows\n",
    "only_df2=compare.df2_unq_rows\n",
    "\n",
    "# only_rest.groupby('hour').count().plot()\n",
    "# only_wss.groupby('hour').count().plot()\n",
    "\n",
    "# only_rest_desc=only_rest.describe()\n",
    "# trade_socket_desc=only_wss.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação unitária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>linkedin_user_name</th>\n",
       "      <th>probability</th>\n",
       "      <th>rating</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>pipefy_last_card_id_link</th>\n",
       "      <th>talent_area</th>\n",
       "      <th>talent_department</th>\n",
       "      <th>talent_expected_salary</th>\n",
       "      <th>talent_hiring_model</th>\n",
       "      <th>...</th>\n",
       "      <th>prospect_seniority_migration</th>\n",
       "      <th>prospect_smart_skills_qty</th>\n",
       "      <th>reason_disapproval</th>\n",
       "      <th>suggestion</th>\n",
       "      <th>talent_expected_salary_interval</th>\n",
       "      <th>talent_info_source</th>\n",
       "      <th>talent_previous_job_id</th>\n",
       "      <th>talent_rank</th>\n",
       "      <th>talent_status</th>\n",
       "      <th>total_experience_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2546</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.165803</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Repensar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11896</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2665</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.168251</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Mid-level</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Repensar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3745</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.204754</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Abordar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7345</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2669</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.159602</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Specialist</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Repensar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16170</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2578</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.168739</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Repensar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11650</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2303</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.264069</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Mid-level</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Abordar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2689</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.242701</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Abordar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1975</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2454</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.178884</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Abordar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7084</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2645</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.124659</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Repensar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7622</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2345</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.175924</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Abordar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2675</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.145911</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Repensar</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2786</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    job_id linkedin_user_name  probability rating  \\\n",
       "0     2546       tecnoricardo     0.165803      7   \n",
       "1     2665       tecnoricardo     0.168251      7   \n",
       "2     2222       tecnoricardo     0.204754      8   \n",
       "3     2669       tecnoricardo     0.159602      7   \n",
       "4     2578       tecnoricardo     0.168739      7   \n",
       "5     2303       tecnoricardo     0.264069      9   \n",
       "6     2689       tecnoricardo     0.242701      9   \n",
       "7     2454       tecnoricardo     0.178884      8   \n",
       "8     2645       tecnoricardo     0.124659      6   \n",
       "9     2345       tecnoricardo     0.175924      8   \n",
       "10    2675       tecnoricardo     0.145911      7   \n",
       "\n",
       "                                    linkedin pipefy_last_card_id_link  \\\n",
       "0   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "1   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "2   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "3   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "4   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "5   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "6   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "7   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "8   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "9   https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "10  https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "\n",
       "   talent_area talent_department  talent_expected_salary talent_hiring_model  \\\n",
       "0    Segurança    Cyber Security                    -1.0             Missing   \n",
       "1    Segurança    Cyber Security                    -1.0             Missing   \n",
       "2    Segurança    Cyber Security                    -1.0             Missing   \n",
       "3    Segurança    Cyber Security                    -1.0             Missing   \n",
       "4    Segurança    Cyber Security                    -1.0             Missing   \n",
       "5    Segurança    Cyber Security                    -1.0             Missing   \n",
       "6    Segurança    Cyber Security                    -1.0             Missing   \n",
       "7    Segurança    Cyber Security                    -1.0             Missing   \n",
       "8    Segurança    Cyber Security                    -1.0             Missing   \n",
       "9    Segurança    Cyber Security                    -1.0             Missing   \n",
       "10   Segurança    Cyber Security                    -1.0             Missing   \n",
       "\n",
       "    ... prospect_seniority_migration prospect_smart_skills_qty  \\\n",
       "0   ...            Missing-to-Senior                         8   \n",
       "1   ...         Missing-to-Mid-level                         8   \n",
       "2   ...            Missing-to-Senior                         8   \n",
       "3   ...        Missing-to-Specialist                         8   \n",
       "4   ...            Missing-to-Senior                         8   \n",
       "5   ...         Missing-to-Mid-level                         8   \n",
       "6   ...            Missing-to-Senior                         8   \n",
       "7   ...            Missing-to-Senior                         8   \n",
       "8   ...            Missing-to-Senior                         8   \n",
       "9   ...            Missing-to-Senior                         8   \n",
       "10  ...            Missing-to-Senior                         8   \n",
       "\n",
       "   reason_disapproval  suggestion  talent_expected_salary_interval  \\\n",
       "0             Missing    Repensar                       u) Missing   \n",
       "1             Missing    Repensar                       u) Missing   \n",
       "2             Missing     Abordar                       u) Missing   \n",
       "3             Missing    Repensar                       u) Missing   \n",
       "4             Missing    Repensar                       u) Missing   \n",
       "5             Missing     Abordar                       u) Missing   \n",
       "6             Missing     Abordar                       u) Missing   \n",
       "7             Missing     Abordar                       u) Missing   \n",
       "8             Missing    Repensar                       u) Missing   \n",
       "9             Missing     Abordar                       u) Missing   \n",
       "10            Missing    Repensar                       u) Missing   \n",
       "\n",
       "    talent_info_source talent_previous_job_id  talent_rank talent_status  \\\n",
       "0            prospects                    NaN        11896     AVAILABLE   \n",
       "1            prospects                    NaN         3745     AVAILABLE   \n",
       "2            prospects                    NaN         7345     AVAILABLE   \n",
       "3            prospects                    NaN        16170     AVAILABLE   \n",
       "4            prospects                    NaN        11650     AVAILABLE   \n",
       "5            prospects                    NaN           26     AVAILABLE   \n",
       "6            prospects                    NaN         1975     AVAILABLE   \n",
       "7            prospects                    NaN         7084     AVAILABLE   \n",
       "8            prospects                    NaN         7622     AVAILABLE   \n",
       "9            prospects                    NaN           17     AVAILABLE   \n",
       "10           prospects                    NaN         2786     AVAILABLE   \n",
       "\n",
       "    total_experience_months  \n",
       "0                       139  \n",
       "1                       139  \n",
       "2                       139  \n",
       "3                       139  \n",
       "4                       139  \n",
       "5                       139  \n",
       "6                       139  \n",
       "7                       139  \n",
       "8                       139  \n",
       "9                       139  \n",
       "10                      139  \n",
       "\n",
       "[11 rows x 79 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>linkedin_user_name</th>\n",
       "      <th>probability</th>\n",
       "      <th>rating</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>pipefy_last_card_id_link</th>\n",
       "      <th>talent_area</th>\n",
       "      <th>talent_department</th>\n",
       "      <th>talent_expected_salary</th>\n",
       "      <th>talent_hiring_model</th>\n",
       "      <th>...</th>\n",
       "      <th>prospect_seniority_migration</th>\n",
       "      <th>prospect_smart_skills_qty</th>\n",
       "      <th>reason_disapproval</th>\n",
       "      <th>talent_expected_salary_interval</th>\n",
       "      <th>talent_info_source</th>\n",
       "      <th>talent_previous_job_id</th>\n",
       "      <th>talent_rank</th>\n",
       "      <th>talent_status</th>\n",
       "      <th>total_experience_months</th>\n",
       "      <th>suggestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2303</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.360057</td>\n",
       "      <td>10</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Mid-level</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2067</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "      <td>Abordar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2345</td>\n",
       "      <td>tecnoricardo</td>\n",
       "      <td>0.251572</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.linkedin.com/in/tecnoricardo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Segurança</td>\n",
       "      <td>Cyber Security</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing-to-Senior</td>\n",
       "      <td>8</td>\n",
       "      <td>Missing</td>\n",
       "      <td>u) Missing</td>\n",
       "      <td>prospects</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>442</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>139</td>\n",
       "      <td>Abordar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id linkedin_user_name  probability rating  \\\n",
       "0    2303       tecnoricardo     0.360057     10   \n",
       "1    2345       tecnoricardo     0.251572      9   \n",
       "\n",
       "                                   linkedin pipefy_last_card_id_link  \\\n",
       "0  https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "1  https://www.linkedin.com/in/tecnoricardo                  Missing   \n",
       "\n",
       "  talent_area talent_department  talent_expected_salary talent_hiring_model  \\\n",
       "0   Segurança    Cyber Security                    -1.0             Missing   \n",
       "1   Segurança    Cyber Security                    -1.0             Missing   \n",
       "\n",
       "   ... prospect_seniority_migration prospect_smart_skills_qty  \\\n",
       "0  ...         Missing-to-Mid-level                         8   \n",
       "1  ...            Missing-to-Senior                         8   \n",
       "\n",
       "  reason_disapproval  talent_expected_salary_interval  talent_info_source  \\\n",
       "0            Missing                       u) Missing           prospects   \n",
       "1            Missing                       u) Missing           prospects   \n",
       "\n",
       "   talent_previous_job_id talent_rank  talent_status total_experience_months  \\\n",
       "0                    <NA>        2067      AVAILABLE                     139   \n",
       "1                    <NA>         442      AVAILABLE                     139   \n",
       "\n",
       "   suggestion  \n",
       "0     Abordar  \n",
       "1     Abordar  \n",
       "\n",
       "[2 rows x 79 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'application_current_phase_active': 'Missing',\n",
       "  'application_hiring_model': 'Missing',\n",
       "  'expected_salary_interval': 'Missing',\n",
       "  'reason_disapproval': 'Missing',\n",
       "  'application_status': 'Missing',\n",
       "  'prev_application_job_area': 'Missing',\n",
       "  'prev_application_job_department': 'Missing',\n",
       "  'last_company_classification': 'Software House C',\n",
       "  'job_area': 'Segurança',\n",
       "  'prev_approach_job_area': 'Missing',\n",
       "  'prev_approach_status': 'Missing',\n",
       "  'company_classification_migration': 'Software House C-to-Empresa Tradicional B',\n",
       "  'talent_info_source': 'prospects',\n",
       "  'prev_approach_job_area_to_job_area': 'Missing-to-Segurança',\n",
       "  'prev_application_job_area_to_job_area': 'Missing-to-Segurança',\n",
       "  'prev_approach_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
       "  'prev_application_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
       "  'prev_approach_job_seniority_to_job_seniority': 'Missing-to-Mid-level',\n",
       "  'prev_application_job_seniority_to_job_seniority': 'Missing-to-Mid-level',\n",
       "  'prev_approach_job_position_to_job_position': 'Missing-to-Missing',\n",
       "  'prev_application_job_position_to_job_position': 'Missing-to-Missing',\n",
       "  'prev_application_job_hiring_type_to_job_hiring_type': 'Missing-to-Missing',\n",
       "  'prospect_seniority_migration': 'Missing-to-Mid-level',\n",
       "  'declared_seniority_migration': 'Missing-to-Mid-level',\n",
       "  'prospect_location_state': 'Ceará',\n",
       "  'prospect_location_region': 'Ceará',\n",
       "  'prospect_area_migration': 'Segurança-to-Segurança',\n",
       "  'previous_applications_qty': 0,\n",
       "  'days_since_last_application': 999,\n",
       "  'prospect_smart_skills_qty': 8,\n",
       "  'prospect_experiences_qty': 5,\n",
       "  'prospect_companies_qty': 5,\n",
       "  'total_experience_months': 139,\n",
       "  'experience_duration_months_min': 5,\n",
       "  'experience_duration_months_clean_avg': 28.0,\n",
       "  'last_experience_duration_months': 27,\n",
       "  'max_salary_offered': 9000.0,\n",
       "  'previous_approaches_qty': 0,\n",
       "  'max_salary_expected_to_prev_approach': 11390.0,\n",
       "  'max_salary_expected_to_prev_application': 11902.33984375,\n",
       "  'last_experience_descriptions_word_count': 32,\n",
       "  'all_company_classifications_word_count': 7,\n",
       "  'all_company_classifications_count': 1,\n",
       "  'all_prospect_smart_skills_word_count': 8,\n",
       "  'all_job_question_correct_answers_word_count': 55,\n",
       "  'all_job_skill_names_word_count': 1,\n",
       "  'import_policy_word_count': 14,\n",
       "  'job_validation_questions_word_count': 179},\n",
       " 1: {'application_current_phase_active': 'Missing',\n",
       "  'application_hiring_model': 'Missing',\n",
       "  'expected_salary_interval': 'Missing',\n",
       "  'reason_disapproval': 'Missing',\n",
       "  'application_status': 'Missing',\n",
       "  'prev_application_job_area': 'Missing',\n",
       "  'prev_application_job_department': 'Missing',\n",
       "  'last_company_classification': 'Software House C',\n",
       "  'job_area': 'Segurança',\n",
       "  'prev_approach_job_area': 'Missing',\n",
       "  'prev_approach_status': 'Missing',\n",
       "  'company_classification_migration': 'Software House C-to-Empresa Tradicional B',\n",
       "  'talent_info_source': 'prospects',\n",
       "  'prev_approach_job_area_to_job_area': 'Missing-to-Segurança',\n",
       "  'prev_application_job_area_to_job_area': 'Missing-to-Segurança',\n",
       "  'prev_approach_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
       "  'prev_application_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
       "  'prev_approach_job_seniority_to_job_seniority': 'Missing-to-Senior',\n",
       "  'prev_application_job_seniority_to_job_seniority': 'Missing-to-Senior',\n",
       "  'prev_approach_job_position_to_job_position': 'Missing-to-Missing',\n",
       "  'prev_application_job_position_to_job_position': 'Missing-to-Missing',\n",
       "  'prev_application_job_hiring_type_to_job_hiring_type': 'Missing-to-Missing',\n",
       "  'prospect_seniority_migration': 'Missing-to-Senior',\n",
       "  'declared_seniority_migration': 'Missing-to-Senior',\n",
       "  'prospect_location_state': 'Ceará',\n",
       "  'prospect_location_region': 'Ceará',\n",
       "  'prospect_area_migration': 'Segurança-to-Segurança',\n",
       "  'previous_applications_qty': 0,\n",
       "  'days_since_last_application': 999,\n",
       "  'prospect_smart_skills_qty': 8,\n",
       "  'prospect_experiences_qty': 5,\n",
       "  'prospect_companies_qty': 5,\n",
       "  'total_experience_months': 139,\n",
       "  'experience_duration_months_min': 5,\n",
       "  'experience_duration_months_clean_avg': 28.0,\n",
       "  'last_experience_duration_months': 27,\n",
       "  'max_salary_offered': 11000.0,\n",
       "  'previous_approaches_qty': 0,\n",
       "  'max_salary_expected_to_prev_approach': 11390.0,\n",
       "  'max_salary_expected_to_prev_application': 11902.33984375,\n",
       "  'last_experience_descriptions_word_count': 32,\n",
       "  'all_company_classifications_word_count': 7,\n",
       "  'all_company_classifications_count': 1,\n",
       "  'all_prospect_smart_skills_word_count': 8,\n",
       "  'all_job_question_correct_answers_word_count': 17,\n",
       "  'all_job_skill_names_word_count': 1,\n",
       "  'import_policy_word_count': 1,\n",
       "  'job_validation_questions_word_count': 211}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_aws[selected_features].to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------------------------------------------+----------+\n",
      "|job_validation_questions_word_count|detailed_prediction                          |prediction|\n",
      "+-----------------------------------+---------------------------------------------+----------+\n",
      "|179                                |{1, {0.7359305587780276, 0.2640694412219724}}|1         |\n",
      "|211                                |{1, {0.8240762848073997, 0.1759237151926003}}|1         |\n",
      "+-----------------------------------+---------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {0: {'application_current_phase_active': 'Missing',\n",
    "  'application_hiring_model': 'Missing',\n",
    "  'expected_salary_interval': 'Missing',\n",
    "  'reason_disapproval': 'Missing',\n",
    "  'application_status': 'Missing',\n",
    "  'prev_application_job_area': 'Missing',\n",
    "  'prev_application_job_department': 'Missing',\n",
    "  'last_company_classification': 'Software House C',\n",
    "  'job_area': 'Segurança',\n",
    "  'prev_approach_job_area': 'Missing',\n",
    "  'prev_approach_status': 'Missing',\n",
    "  'company_classification_migration': 'Software House C-to-Empresa Tradicional B',\n",
    "  'talent_info_source': 'prospects',\n",
    "  'prev_approach_job_area_to_job_area': 'Missing-to-Segurança',\n",
    "  'prev_application_job_area_to_job_area': 'Missing-to-Segurança',\n",
    "  'prev_approach_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
    "  'prev_application_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
    "  'prev_approach_job_seniority_to_job_seniority': 'Missing-to-Mid-level',\n",
    "  'prev_application_job_seniority_to_job_seniority': 'Missing-to-Mid-level',\n",
    "  'prev_approach_job_position_to_job_position': 'Missing-to-Missing',\n",
    "  'prev_application_job_position_to_job_position': 'Missing-to-Missing',\n",
    "  'prev_application_job_hiring_type_to_job_hiring_type': 'Missing-to-Missing',\n",
    "  'prospect_seniority_migration': 'Missing-to-Mid-level',\n",
    "  'declared_seniority_migration': 'Missing-to-Mid-level',\n",
    "  'prospect_location_state': 'Ceará',\n",
    "  'prospect_location_region': 'Ceará',\n",
    "  'prospect_area_migration': 'Segurança-to-Segurança',\n",
    "  'previous_applications_qty': 0,\n",
    "  'days_since_last_application': 999,\n",
    "  'prospect_smart_skills_qty': 8,\n",
    "  'prospect_experiences_qty': 5,\n",
    "  'prospect_companies_qty': 5,\n",
    "  'total_experience_months': 139,\n",
    "  'experience_duration_months_min': 5,\n",
    "  'experience_duration_months_clean_avg': 28.0,\n",
    "  'last_experience_duration_months': 27,\n",
    "  'max_salary_offered': 9000.0,\n",
    "  'previous_approaches_qty': 0,\n",
    "  'max_salary_offered_to_prev_approach': 11390.0,\n",
    "  'max_salary_offered_to_prev_application': 11902.33984375,\n",
    "  'last_experience_descriptions_word_count': 32,\n",
    "  'all_company_classifications_word_count': 7,\n",
    "  'all_company_classifications_count': 1,\n",
    "  'all_prospect_smart_skills_word_count': 8,\n",
    "  'all_job_question_correct_answers_word_count': 55,\n",
    "  'all_job_skill_names_word_count': 1,\n",
    "  'import_policy_word_count': 14,\n",
    "  'job_validation_questions_word_count': 179},\n",
    " 1: {'application_current_phase_active': 'Missing',\n",
    "  'application_hiring_model': 'Missing',\n",
    "  'expected_salary_interval': 'Missing',\n",
    "  'reason_disapproval': 'Missing',\n",
    "  'application_status': 'Missing',\n",
    "  'prev_application_job_area': 'Missing',\n",
    "  'prev_application_job_department': 'Missing',\n",
    "  'last_company_classification': 'Software House C',\n",
    "  'job_area': 'Segurança',\n",
    "  'prev_approach_job_area': 'Missing',\n",
    "  'prev_approach_status': 'Missing',\n",
    "  'company_classification_migration': 'Software House C-to-Empresa Tradicional B',\n",
    "  'talent_info_source': 'prospects',\n",
    "  'prev_approach_job_area_to_job_area': 'Missing-to-Segurança',\n",
    "  'prev_application_job_area_to_job_area': 'Missing-to-Segurança',\n",
    "  'prev_approach_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
    "  'prev_application_company_classification_migration': 'Missing-to-Empresa Tradicional B',\n",
    "  'prev_approach_job_seniority_to_job_seniority': 'Missing-to-Senior',\n",
    "  'prev_application_job_seniority_to_job_seniority': 'Missing-to-Senior',\n",
    "  'prev_approach_job_position_to_job_position': 'Missing-to-Missing',\n",
    "  'prev_application_job_position_to_job_position': 'Missing-to-Missing',\n",
    "  'prev_application_job_hiring_type_to_job_hiring_type': 'Missing-to-Missing',\n",
    "  'prospect_seniority_migration': 'Missing-to-Senior',\n",
    "  'declared_seniority_migration': 'Missing-to-Senior',\n",
    "  'prospect_location_state': 'Ceará',\n",
    "  'prospect_location_region': 'Ceará',\n",
    "  'prospect_area_migration': 'Segurança-to-Segurança',\n",
    "  'previous_applications_qty': 0,\n",
    "  'days_since_last_application': 999,\n",
    "  'prospect_smart_skills_qty': 8,\n",
    "  'prospect_experiences_qty': 5,\n",
    "  'prospect_companies_qty': 5,\n",
    "  'total_experience_months': 139,\n",
    "  'experience_duration_months_min': 5,\n",
    "  'experience_duration_months_clean_avg': 28.0,\n",
    "  'last_experience_duration_months': 27,\n",
    "  'max_salary_offered': 11000.0,\n",
    "  'previous_approaches_qty': 0,\n",
    "  'max_salary_offered_to_prev_approach': 11390.0,\n",
    "  'max_salary_offered_to_prev_application': 11902.33984375,\n",
    "  'last_experience_descriptions_word_count': 32,\n",
    "  'all_company_classifications_word_count': 7,\n",
    "  'all_company_classifications_count': 1,\n",
    "  'all_prospect_smart_skills_word_count': 8,\n",
    "  'all_job_question_correct_answers_word_count': 17,\n",
    "  'all_job_skill_names_word_count': 1,\n",
    "  'import_policy_word_count': 1,\n",
    "  'job_validation_questions_word_count': 211}}\n",
    "\n",
    "sdf = (spark.createDataFrame(pd.DataFrame.from_dict(data, orient='index')))\n",
    "\n",
    "## Escorar a base, criar o score e o rating\n",
    "settings = H2OMOJOSettings(convertUnknownCategoricalLevelsToNa = True, convertInvalidNumbersToNa = True)\n",
    "model = H2OMOJOModel.createFromMojo(PathModelMojo, settings)\n",
    "sherlock_smart_list = model.transform(sdf)\n",
    "\n",
    "# sherlock_smart_list.printSchema()\n",
    "sherlock_smart_list.select('job_validation_questions_word_count', 'detailed_prediction', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustText==0.7.3\n",
      "alabaster==0.7.12\n",
      "appdirs==1.4.4\n",
      "argon2-cffi==20.1.0\n",
      "asn1crypto==1.4.0\n",
      "astroid==2.4.2\n",
      "atomicwrites==1.4.0\n",
      "attrs==20.3.0\n",
      "autobahn==20.7.1\n",
      "Automat==20.2.0\n",
      "autopep8==1.5.4\n",
      "awswrangler==2.8.0\n",
      "Babel==2.8.0\n",
      "backcall==0.2.0\n",
      "bcrypt==3.2.0\n",
      "beautifulsoup4==4.9.1\n",
      "bioinfokit==2.0.8\n",
      "bleach==3.1.5\n",
      "boto==2.49.0\n",
      "boto3==1.21.29\n",
      "botocore==1.24.29\n",
      "Bottleneck==1.3.4\n",
      "bs4==0.0.1\n",
      "certifi==2020.6.20\n",
      "cffi==1.14.2\n",
      "chardet==3.0.4\n",
      "click==8.1.2\n",
      "click-plugins==1.1.1\n",
      "cligj==0.7.2\n",
      "cloudpickle==1.5.0\n",
      "cmdstanpy==0.9.68\n",
      "colorama==0.4.3\n",
      "confuse==1.3.0\n",
      "constantly==15.1.0\n",
      "convertdate==2.4.0\n",
      "cryptography==3.0\n",
      "cycler==0.10.0\n",
      "Cython==0.29.21\n",
      "datacompy==0.7.1\n",
      "dateparser==0.7.6\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "descartes==1.1.0\n",
      "diff-match-patch==20200713\n",
      "dill==0.3.4\n",
      "docopt==0.6.2\n",
      "docutils==0.16\n",
      "entrypoints==0.3\n",
      "ephem==4.1.3\n",
      "et-xmlfile==1.1.0\n",
      "findspark==2.0.1\n",
      "Fiona @ file:///C:/Users/maxde/pipwin/Fiona-1.8.21-cp37-cp37m-win_amd64.whl\n",
      "flake8==2.3.0\n",
      "future==0.18.2\n",
      "GDAL @ file:///C:/Users/maxde/pipwin/GDAL-3.4.2-cp37-cp37m-win_amd64.whl\n",
      "geojson==2.5.0\n",
      "geopandas @ file:///C:/Users/maxde/pipwin/geopandas-0.6.2-py2.py3-none-any.whl\n",
      "google-pasta==0.2.0\n",
      "graphviz==0.14.1\n",
      "greenlet==1.0.0\n",
      "h2o==3.36.1.2\n",
      "h2o-pysparkling-3.1==3.36.1.3.post1\n",
      "h2o-pysparkling-3.3==3.36.1.3.post1\n",
      "HeapDict==1.0.1\n",
      "helpdev==0.7.1\n",
      "hijri-converter==2.2.3\n",
      "holidays==0.13\n",
      "htmlmin==0.1.12\n",
      "hummingbird-ml==0.4.3\n",
      "hyperlink==20.0.1\n",
      "idna==2.10\n",
      "ImageHash==4.1.0\n",
      "imagesize==1.2.0\n",
      "importlib-metadata==1.7.0\n",
      "incremental==17.5.0\n",
      "intervaltree==3.1.0\n",
      "ipykernel==5.3.4\n",
      "ipython==7.17.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "isort==5.4.2\n",
      "jedi==0.15.2\n",
      "Jinja2==2.11.2\n",
      "jmespath==0.10.0\n",
      "joblib==0.16.0\n",
      "Js2Py==0.71\n",
      "json5==0.9.5\n",
      "jsonschema==3.2.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==6.1.6\n",
      "jupyter-console==6.1.0\n",
      "jupyter-core==4.6.3\n",
      "jupyterlab==2.2.5\n",
      "jupyterlab-server==1.2.0\n",
      "keyring==21.3.0\n",
      "kiwisolver==1.2.0\n",
      "korean-lunar-calendar==0.2.1\n",
      "lab==6.0\n",
      "lazy-object-proxy==1.4.3\n",
      "lightgbm==3.3.2\n",
      "llvmlite==0.34.0\n",
      "locket==0.2.1\n",
      "LunarCalendar==0.0.9\n",
      "lxml==4.6.3\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.2.1\n",
      "matplotlib-venn==0.11.6\n",
      "mccabe==0.6.1\n",
      "missingno==0.4.2\n",
      "mistune==0.8.4\n",
      "mpmath==1.1.0\n",
      "multiprocess==0.70.12.2\n",
      "munch==2.5.0\n",
      "mysql==0.0.2\n",
      "mysql-connector-python-rf==2.2.2\n",
      "mysqlclient==2.0.3\n",
      "nbconvert==5.6.1\n",
      "nbformat==5.0.7\n",
      "networkx==2.5\n",
      "notebook==6.1.3\n",
      "numba==0.51.2\n",
      "numpy==1.19.1\n",
      "numpydoc==1.1.0\n",
      "onnx==1.11.0\n",
      "onnxconverter-common==1.9.0\n",
      "openpyxl==3.0.7\n",
      "packaging==21.3\n",
      "pandas==1.2.3\n",
      "pandas-datareader==0.9.0\n",
      "pandas-lite==0.1.1\n",
      "pandas-profiling @ https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
      "pandas-ta==0.2.23b0\n",
      "pandocfilters==1.4.2\n",
      "paramiko==2.7.1\n",
      "parso==0.5.2\n",
      "partd==1.1.0\n",
      "pathos==0.2.8\n",
      "pathtools==0.1.2\n",
      "patsy==0.5.1\n",
      "pep8==1.7.1\n",
      "pexpect==4.8.0\n",
      "pg8000==1.19.5\n",
      "phik==0.10.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==7.2.0\n",
      "pipdate==0.5.2\n",
      "pipwin==0.5.2\n",
      "plotly==4.14.3\n",
      "pluggy==0.13.1\n",
      "pox==0.3.0\n",
      "ppft==1.6.6.4\n",
      "prometheus-client==0.8.0\n",
      "prompt-toolkit==3.0.6\n",
      "protobuf==3.19.4\n",
      "protobuf3-to-dict==0.1.5\n",
      "psutil==5.7.2\n",
      "psycopg2==2.8.6\n",
      "ptyprocess==0.6.0\n",
      "py-instagram-dl==1.5\n",
      "py4j==0.10.9\n",
      "pyarrow==4.0.1\n",
      "pyasn1==0.4.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -2o (c:\\program files\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\program files\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\program files\\python37\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyasn1-modules==0.2.8\n",
      "pyathena==2.3.0\n",
      "pycodestyle==2.6.0\n",
      "pycparser==2.20\n",
      "pydocstyle==5.0.2\n",
      "pyflakes==2.1.1\n",
      "Pygments==2.6.1\n",
      "PyHamcrest==2.0.2\n",
      "pyjsparser==2.7.1\n",
      "pylint==2.6.0\n",
      "pyls==0.1.5\n",
      "PyMeeus==0.5.11\n",
      "PyMySQL==1.0.2\n",
      "PyNaCl==1.4.0\n",
      "pyOpenSSL==19.1.0\n",
      "pyparsing==2.4.7\n",
      "PyPrind==2.11.3\n",
      "pyproj @ file:///C:/Users/maxde/pipwin/pyproj-3.2.1-cp37-cp37m-win_amd64.whl\n",
      "PyQt5==5.12.3\n",
      "PyQt5-sip==12.8.0\n",
      "PyQtWebEngine==5.12.1\n",
      "pyrsistent==0.16.0\n",
      "pySmartDL==1.3.4\n",
      "pyspark==3.1.1\n",
      "pystan==2.19.1.1\n",
      "python-binance==0.7.5\n",
      "python-dateutil==2.8.1\n",
      "python-jsonrpc-server==0.3.4\n",
      "python-language-server==0.31.10\n",
      "pytz==2020.1\n",
      "PyWavelets==1.1.1\n",
      "pywin32==228\n",
      "pywin32-ctypes==0.2.0\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.3.1\n",
      "pyzmq==19.0.2\n",
      "QDarkStyle==2.8.1\n",
      "QtAwesome==0.7.2\n",
      "qtconsole==4.7.6\n",
      "QtPy==1.9.0\n",
      "redshift-connector==2.0.881\n",
      "regex==2020.7.14\n",
      "requests==2.24.0\n",
      "requests-toolbelt==0.9.1\n",
      "retrying==1.3.3\n",
      "rockset==0.8.10\n",
      "rope==0.17.0\n",
      "Rtree @ file:///C:/Users/maxde/pipwin/Rtree-0.9.7-cp37-cp37m-win_amd64.whl\n",
      "s3transfer==0.5.2\n",
      "sagemaker==2.81.1\n",
      "scikit-learn==0.23.2\n",
      "scipy==1.5.2\n",
      "scramp==1.4.0\n",
      "seaborn==0.11.0\n",
      "Send2Trash==1.5.0\n",
      "service-identity==18.1.0\n",
      "setuptools-git==1.2\n",
      "shap==0.37.0\n",
      "shap2==0.40.0\n",
      "Shapely==1.8.1.post1\n",
      "simplejson==3.17.2\n",
      "six @ file:///C:/Users/maxde/pipwin/six-1.16.0-py3-none-any.whl\n",
      "sklearn==0.0\n",
      "slicer==0.0.7\n",
      "smdebug-rulesconfig==1.0.1\n",
      "snowballstemmer==2.0.0\n",
      "sortedcontainers==2.2.2\n",
      "soupsieve==2.0.1\n",
      "Sphinx==3.2.1\n",
      "sphinxcontrib-applehelp==1.0.2\n",
      "sphinxcontrib-devhelp==1.0.2\n",
      "sphinxcontrib-htmlhelp==1.0.3\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-qthelp==1.0.3\n",
      "sphinxcontrib-serializinghtml==1.1.4\n",
      "spyder==4.1.3\n",
      "spyder-kernels==1.9.3\n",
      "SQLAlchemy==1.4.12\n",
      "statsmodels==0.12.2\n",
      "sympy==1.6.2\n",
      "tabulate==0.8.7\n",
      "tangled-up-in-unicode==0.0.6\n",
      "tblib==1.7.0\n",
      "tenacity==7.0.0\n",
      "terminado==0.8.3\n",
      "testpath==0.4.4\n",
      "texttable==1.6.4\n",
      "textwrap3==0.9.2\n",
      "threadpoolctl==2.1.0\n",
      "tinys3==0.1.12\n",
      "toml==0.10.1\n",
      "toolz==0.11.1\n",
      "torch==1.11.0\n",
      "tornado==6.0.4\n",
      "tqdm==4.51.0\n",
      "traitlets==4.3.3\n",
      "Twisted==20.3.0\n",
      "txaio==20.4.1\n",
      "txt2tags==3.7\n",
      "typed-ast==1.4.1\n",
      "typing_extensions==4.1.1\n",
      "tzlocal==2.1\n",
      "ujson==5.2.0\n",
      "urllib3==1.25.10\n",
      "visions==0.5.0\n",
      "watchdog==0.10.3\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "widgetsnbextension==3.5.1\n",
      "wrapt==1.12.1\n",
      "xgboost==1.2.1\n",
      "yapf==0.30.0\n",
      "zict==2.0.0\n",
      "zipp==3.1.0\n",
      "zope.interface==5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'application_current_phase_active': 'Missing', 'application_hiring_model': 'Missing', 'expected_salary_interval': 'Missing', 'reason_disapproval': 'Missing', 'application_status': 'Missing', 'prev_application_job_area': 'Missing', 'prev_application_job_department': 'Missing', 'last_company_classification': 'Software House C', 'job_area': 'Segurança', 'prev_approach_job_area': 'Missing', 'prev_approach_status': 'Missing', 'company_classification_migration': 'Software House C-to-Empresa Tradicional B', 'talent_info_source': 'prospects', 'prev_approach_job_area_to_job_area': 'Missing-to-Segurança', 'prev_application_job_area_to_job_area': 'Missing-to-Segurança', 'prev_approach_company_classification_migration': 'Missing-to-Empresa Tradicional B', 'prev_application_company_classification_migration': 'Missing-to-Empresa Tradicional B', 'prev_approach_job_seniority_to_job_seniority': 'Missing-to-Mid-level', 'prev_application_job_seniority_to_job_seniority': 'Missing-to-Mid-level', 'prev_approach_job_position_to_job_position': 'Missing-to-Missing', 'prev_application_job_position_to_job_position': 'Missing-to-Missing', 'prev_application_job_hiring_type_to_job_hiring_type': 'Missing-to-Missing', 'prospect_seniority_migration': 'Missing-to-Mid-level', 'declared_seniority_migration': 'Missing-to-Mid-level', 'prospect_location_state': 'Ceará', 'prospect_location_region': 'Ceará', 'prospect_area_migration': 'Segurança-to-Segurança', 'previous_applications_qty': 0, 'days_since_last_application': 999, 'prospect_smart_skills_qty': 8, 'prospect_experiences_qty': 5, 'prospect_companies_qty': 5, 'total_experience_months': 139, 'experience_duration_months_min': 5, 'experience_duration_months_clean_avg': 28.0, 'last_experience_duration_months': 27, 'max_salary_offered': 9000.0, 'previous_approaches_qty': 0, 'max_salary_expected_to_prev_approach': 11390.0, 'max_salary_expected_to_prev_application': 11902.33984375, 'last_experience_descriptions_word_count': 32, 'all_company_classifications_word_count': 7, 'all_company_classifications_count': 1, 'all_prospect_smart_skills_word_count': 8, 'all_job_question_correct_answers_word_count': 55, 'all_job_skill_names_word_count': 1, 'import_policy_word_count': 14, 'job_validation_questions_word_count': 179}}\n"
     ]
    }
   ],
   "source": [
    "print(sdf.limit(1).toPandas().to_dict(orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{0: {'application_current_phase_active': 'Missing', 'application_hiring_model': 'Missing', 'expected_salary_interval': 'Missing', 'reason_disapproval': 'Missing', 'application_status': 'Missing', 'prev_application_job_area': 'Missing', 'prev_application_job_department': 'Missing', 'last_company_classification': 'Software House C', 'job_area': 'Segurança', 'prev_approach_job_area': 'Missing', 'prev_approach_status': 'Missing', 'company_classification_migration': 'Software House C-to-Empresa Tradicional B', 'talent_info_source': 'prospects', 'prev_approach_job_area_to_job_area': 'Missing-to-Segurança', 'prev_application_job_area_to_job_area': 'Missing-to-Segurança', 'prev_approach_company_classification_migration': 'Missing-to-Empresa Tradicional B', 'prev_application_company_classification_migration': 'Missing-to-Empresa Tradicional B', 'prev_approach_job_seniority_to_job_seniority': 'Missing-to-Mid-level', 'prev_application_job_seniority_to_job_seniority': 'Missing-to-Mid-level', 'prev_approach_job_position_to_job_position': 'Missing-to-Missing', 'prev_application_job_position_to_job_position': 'Missing-to-Missing', 'prev_application_job_hiring_type_to_job_hiring_type': 'Missing-to-Missing', 'prospect_seniority_migration': 'Missing-to-Mid-level', 'declared_seniority_migration': 'Missing-to-Mid-level', 'prospect_location_state': 'Ceará', 'prospect_location_region': 'Ceará', 'prospect_area_migration': 'Segurança-to-Segurança', 'previous_applications_qty': 0, 'days_since_last_application': 999, 'prospect_smart_skills_qty': 8, 'prospect_experiences_qty': 5, 'prospect_companies_qty': 5, 'total_experience_months': 139, 'experience_duration_months_min': 5, 'experience_duration_months_clean_avg': 28.0, 'last_experience_duration_months': 27, 'max_salary_offered': 9000.0, 'previous_approaches_qty': 0, 'max_salary_offered_to_prev_approach': 11390.0, 'max_salary_offered_to_prev_application': 11902.33984375, 'last_experience_descriptions_word_count': 32, 'all_company_classifications_word_count': 7, 'all_company_classifications_count': 1, 'all_prospect_smart_skills_word_count': 8, 'all_job_question_correct_answers_word_count': 55, 'all_job_skill_names_word_count': 1, 'import_policy_word_count': 14, 'job_validation_questions_word_count': 179}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "ai.h2o.sparkling.utils.JSONDataFrameSerializer\n",
      "detailed_prediction\n",
      "True\n",
      "prediction\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(settings.convertInvalidNumbersToNa)\n",
    "print(settings.convertUnknownCategoricalLevelsToNa)\n",
    "print(settings.dataFrameSerializer)\n",
    "print(settings.detailedPredictionCol)\n",
    "print(settings.namedMojoOutputColumns)\n",
    "print(settings.predictionCol)\n",
    "print(settings.withContributions)\n",
    "print(settings.withLeafNodeAssignments)\n",
    "print(settings.withStageResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'StackedEnsemble_BestOfFamily_3_AutoML_1_20220713_190002.zip_411de2db0010'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.defaultThreshold.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- application_current_phase_active: string (nullable = true)\n",
      " |-- application_hiring_model: string (nullable = true)\n",
      " |-- expected_salary_interval: string (nullable = true)\n",
      " |-- reason_disapproval: string (nullable = true)\n",
      " |-- application_status: string (nullable = true)\n",
      " |-- prev_application_job_area: string (nullable = true)\n",
      " |-- prev_application_job_department: string (nullable = true)\n",
      " |-- last_company_classification: string (nullable = true)\n",
      " |-- job_area: string (nullable = true)\n",
      " |-- prev_approach_job_area: string (nullable = true)\n",
      " |-- prev_approach_status: string (nullable = true)\n",
      " |-- company_classification_migration: string (nullable = true)\n",
      " |-- talent_info_source: string (nullable = true)\n",
      " |-- prev_approach_job_area_to_job_area: string (nullable = true)\n",
      " |-- prev_application_job_area_to_job_area: string (nullable = true)\n",
      " |-- prev_approach_company_classification_migration: string (nullable = true)\n",
      " |-- prev_application_company_classification_migration: string (nullable = true)\n",
      " |-- prev_approach_job_seniority_to_job_seniority: string (nullable = true)\n",
      " |-- prev_application_job_seniority_to_job_seniority: string (nullable = true)\n",
      " |-- prev_approach_job_position_to_job_position: string (nullable = true)\n",
      " |-- prev_application_job_position_to_job_position: string (nullable = true)\n",
      " |-- prev_application_job_hiring_type_to_job_hiring_type: string (nullable = true)\n",
      " |-- prospect_seniority_migration: string (nullable = true)\n",
      " |-- declared_seniority_migration: string (nullable = true)\n",
      " |-- prospect_location_state: string (nullable = true)\n",
      " |-- prospect_location_region: string (nullable = true)\n",
      " |-- prospect_area_migration: string (nullable = true)\n",
      " |-- previous_applications_qty: long (nullable = true)\n",
      " |-- days_since_last_application: long (nullable = true)\n",
      " |-- prospect_smart_skills_qty: long (nullable = true)\n",
      " |-- prospect_experiences_qty: long (nullable = true)\n",
      " |-- prospect_companies_qty: long (nullable = true)\n",
      " |-- total_experience_months: long (nullable = true)\n",
      " |-- experience_duration_months_min: long (nullable = true)\n",
      " |-- experience_duration_months_clean_avg: double (nullable = true)\n",
      " |-- last_experience_duration_months: long (nullable = true)\n",
      " |-- max_salary_offered: double (nullable = true)\n",
      " |-- previous_approaches_qty: long (nullable = true)\n",
      " |-- max_salary_expected_to_prev_approach: double (nullable = true)\n",
      " |-- max_salary_expected_to_prev_application: double (nullable = true)\n",
      " |-- last_experience_descriptions_word_count: long (nullable = true)\n",
      " |-- all_company_classifications_word_count: long (nullable = true)\n",
      " |-- all_company_classifications_count: long (nullable = true)\n",
      " |-- all_prospect_smart_skills_word_count: long (nullable = true)\n",
      " |-- all_job_question_correct_answers_word_count: long (nullable = true)\n",
      " |-- all_job_skill_names_word_count: long (nullable = true)\n",
      " |-- import_policy_word_count: long (nullable = true)\n",
      " |-- job_validation_questions_word_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = (spark.createDataFrame(pd.DataFrame.from_dict(data, orient='index')))\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode)\n",
      "  Starting server from c:\\program files\\python37\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\maxde\\AppData\\Local\\Temp\\tmpaq998kvi\n",
      "  JVM stdout: C:\\Users\\maxde\\AppData\\Local\\Temp\\tmpaq998kvi\\h2o_maxde_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\maxde\\AppData\\Local\\Temp\\tmpaq998kvi\\h2o_maxde_started_from_python.err\n"
     ]
    },
    {
     "ename": "H2OServerError",
     "evalue": "Server process terminated with error code 1: ERROR: failed in createLog4j, exiting now.\njava.lang.NullPointerException\n\tat water.util.Log.createLog4j(Log.java:349)\n\tat water.util.Log.write0(Log.java:139)\n\tat water.util.Log.flushBufferedMessages(Log.java:156)\n\tat water.util.Log.notifyAboutProcessExiting(Log.java:78)\n\tat water.H2O.exit(H2O.java:948)\n\tat water.H2O.parseFailed(H2O.java:465)\n\tat water.H2O.parseH2OArgumentsTo(H2O.java:760)\n\tat water.H2O.parseArguments(H2O.java:529)\n\tat water.H2O.main(H2O.java:2184)\n\tat water.H2OStarter.start(H2OStarter.java:22)\n\tat water.H2OStarter.start(H2OStarter.java:48)\n\tat water.H2OApp.main(H2OApp.java:12)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mH2OConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\h2o\\h2o.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m                                            \"connected.\", \"not found.\"),\n\u001b[1;32m--> 275\u001b[1;33m                                      strict_version_check=svc)\n\u001b[0m\u001b[0;32m    276\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mH2OConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\h2o\\backend\\connection.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, cacert, proxy, cookies, verbose, msgs, strict_version_check)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m             \u001b[1;31m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\h2o\\backend\\connection.py\u001b[0m in \u001b[0;36m_test_connection\u001b[1;34m(self, max_retries, messages)\u001b[0m\n\u001b[0;32m    690\u001b[0m             raise H2OConnectionError(\"Could not establish link to the H2O cloud %s after %d retries\\n%s\"\n\u001b[1;32m--> 691\u001b[1;33m                                      % (self._base_url, max_retries, \"\\n\".join(errors)))\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mH2OConnectionError\u001b[0m: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries\n[16:53.45] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020ACF5491C8>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\n[16:57.75] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020ACF54D4C8>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\n[17:02.08] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020ACF551488>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\n[17:06.36] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020ACF558448>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\n[17:10.67] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020ACF551908>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mH2OServerError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-95453bf1556d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh2o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\h2o\\h2o.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m                                   \u001b[0mmax_log_file_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_log_file_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                                   \u001b[0mextra_classpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_classpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm_custom_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjvm_custom_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m                                   bind_to_localhost=bind_to_localhost)\n\u001b[0m\u001b[0;32m    292\u001b[0m         h2oconn = H2OConnection.open(server=hs, https=https, verify_ssl_certificates=verify_ssl_certificates,\n\u001b[0;32m    293\u001b[0m                                      \u001b[0mcacert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcacert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\h2o\\backend\\server.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, log_dir, log_level, max_log_file_size, port, name, extra_classpath, verbose, jvm_custom_args, bind_to_localhost)\u001b[0m\n\u001b[0;32m    140\u001b[0m         hs._launch_server(port=port, baseport=baseport, nthreads=int(nthreads), ea=enable_assertions,\n\u001b[0;32m    141\u001b[0m                           \u001b[0mmmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_mem_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_mem_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm_custom_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjvm_custom_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                           bind_to_localhost=bind_to_localhost, log_dir=log_dir, log_level=log_level, max_log_file_size=max_log_file_size)\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  Server is running at %s://%s:%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0matexit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\h2o\\backend\\server.py\u001b[0m in \u001b[0;36m_launch_server\u001b[1;34m(self, port, baseport, mmax, mmin, ea, nthreads, jvm_custom_args, bind_to_localhost, log_dir, log_level, max_log_file_size)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                     \u001b[0merror_message\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mH2OServerError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_server_info_from_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mH2OServerError\u001b[0m: Server process terminated with error code 1: ERROR: failed in createLog4j, exiting now.\njava.lang.NullPointerException\n\tat water.util.Log.createLog4j(Log.java:349)\n\tat water.util.Log.write0(Log.java:139)\n\tat water.util.Log.flushBufferedMessages(Log.java:156)\n\tat water.util.Log.notifyAboutProcessExiting(Log.java:78)\n\tat water.H2O.exit(H2O.java:948)\n\tat water.H2O.parseFailed(H2O.java:465)\n\tat water.H2O.parseH2OArgumentsTo(H2O.java:760)\n\tat water.H2O.parseArguments(H2O.java:529)\n\tat water.H2O.main(H2O.java:2184)\n\tat water.H2OStarter.start(H2OStarter.java:22)\n\tat water.H2OStarter.start(H2OStarter.java:48)\n\tat water.H2OApp.main(H2OApp.java:12)\n"
     ]
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Java\\jre1.8.0_261\n"
     ]
    }
   ],
   "source": [
    "!echo %JAVA_HOME%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hdf = h2o.H2OFrame(dataprep_df.query('partition_0 == \"train_data\"').loc[:, (selected_features+[VarTarget])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_local = sherlock_smart_list3.where(F.col('linkedin_user_name')=='tecnoricardo').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select *\n",
    "from \"dev-lakehouse-mirror\".sherlock_smart_list where linkedin_user_name = 'tecnoricardo'\"\"\"\n",
    "user_aws = wr.athena.read_sql_query(query, database=\"dev-lakehouse-mirror\", boto3_session=my_boto3_session, s3_output='s3://query-temp-result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_aws[['prospect_location_state', 'prospect_location_region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataComPy Comparison\n",
      "--------------------\n",
      "\n",
      "DataFrame Summary\n",
      "-----------------\n",
      "\n",
      "    DataFrame  Columns  Rows\n",
      "0  user_local       79    11\n",
      "1    user_aws       79     2\n",
      "\n",
      "Column Summary\n",
      "--------------\n",
      "\n",
      "Number of columns in common: 79\n",
      "Number of columns in user_local but not in user_aws: 0\n",
      "Number of columns in user_aws but not in user_local: 0\n",
      "\n",
      "Row Summary\n",
      "-----------\n",
      "\n",
      "Matched on: job_id, linkedin_user_name\n",
      "Any duplicates on match values: No\n",
      "Absolute Tolerance: 0\n",
      "Relative Tolerance: 0\n",
      "Number of rows in common: 2\n",
      "Number of rows in user_local but not in user_aws: 9\n",
      "Number of rows in user_aws but not in user_local: 0\n",
      "\n",
      "Number of rows with some compared columns unequal: 2\n",
      "Number of rows with all compared columns equal: 0\n",
      "\n",
      "Column Comparison\n",
      "-----------------\n",
      "\n",
      "Number of columns compared with some values unequal: 6\n",
      "Number of columns compared with all values equal: 73\n",
      "Total number of values which compare unequal: 12\n",
      "\n",
      "Columns with Unequal Values or Types\n",
      "------------------------------------\n",
      "\n",
      "                      Column user_local dtype user_aws dtype  # Unequal     Max Diff  # Null Diff\n",
      "0  all_prospect_smart_skills           object         object          2     0.000000            0\n",
      "4                probability           object         object          2     0.095987            0\n",
      "2                     rating           object         object          2     1.000000            0\n",
      "5     talent_previous_job_id           object         object          2     0.000000            0\n",
      "3                talent_rank           object         object          2  2041.000000            0\n",
      "1              talent_skills           object         object          2     0.000000            0\n",
      "\n",
      "Sample Rows with Unequal Values\n",
      "-------------------------------\n",
      "\n",
      "  job_id linkedin_user_name                        all_prospect_smart_skills (user_local)                          all_prospect_smart_skills (user_aws)\n",
      "5   2303       tecnoricardo  cisco,dns,linux-,microsoft-office,smtp,tcp-ip,vmware,windows  linux-,windows,vmware,microsoft-office,dns,smtp,tcp-ip,cisco\n",
      "9   2345       tecnoricardo  cisco,dns,linux-,microsoft-office,smtp,tcp-ip,vmware,windows  linux-,windows,vmware,microsoft-office,dns,smtp,tcp-ip,cisco\n",
      "\n",
      "  job_id linkedin_user_name                                    talent_skills (user_local)                                      talent_skills (user_aws)\n",
      "5   2303       tecnoricardo  cisco,dns,linux-,microsoft-office,smtp,tcp-ip,vmware,windows  linux-,windows,vmware,microsoft-office,dns,smtp,tcp-ip,cisco\n",
      "9   2345       tecnoricardo  cisco,dns,linux-,microsoft-office,smtp,tcp-ip,vmware,windows  linux-,windows,vmware,microsoft-office,dns,smtp,tcp-ip,cisco\n",
      "\n",
      "  job_id linkedin_user_name rating (user_local) rating (user_aws)\n",
      "9   2345       tecnoricardo                   8                 9\n",
      "5   2303       tecnoricardo                   9                10\n",
      "\n",
      "  job_id linkedin_user_name talent_rank (user_local) talent_rank (user_aws)\n",
      "5   2303       tecnoricardo                       26                   2067\n",
      "9   2345       tecnoricardo                       17                    442\n",
      "\n",
      "  job_id linkedin_user_name probability (user_local) probability (user_aws)\n",
      "5   2303       tecnoricardo       0.2640694412764003      0.360056778778978\n",
      "9   2345       tecnoricardo      0.17592371525781747    0.25157175293904127\n",
      "\n",
      "  job_id linkedin_user_name talent_previous_job_id (user_local) talent_previous_job_id (user_aws)\n",
      "9   2345       tecnoricardo                                 nan                              <NA>\n",
      "5   2303       tecnoricardo                                 nan                              <NA>\n",
      "\n",
      "Sample Rows Only in user_local (First 10 Columns)\n",
      "-------------------------------------------------\n",
      "\n",
      "   job_id linkedin_user_name          probability rating                                  linkedin pipefy_last_card_id_link talent_area talent_department talent_expected_salary talent_hiring_model\n",
      "7    2454       tecnoricardo  0.17888443792602257      8  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "6    2689       tecnoricardo  0.24270101656122967      9  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "2    2222       tecnoricardo  0.20475440226879393      8  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "0    2546       tecnoricardo  0.16580270511284106      7  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "8    2645       tecnoricardo  0.12465889223519448      6  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "3    2669       tecnoricardo  0.15960161664269787      7  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "4    2578       tecnoricardo    0.168738744637447      7  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "1    2665       tecnoricardo  0.16825114229185326      7  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "10   2675       tecnoricardo     0.14591095204139      7  https://www.linkedin.com/in/tecnoricardo                  Missing   Segurança    Cyber Security                   -1.0             Missing\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datacompy\n",
    "#comparacao das duas bases\n",
    "compare = datacompy.Compare(\n",
    "    user_local.astype(str),\n",
    "    user_aws.astype(str),\n",
    "    join_columns=['job_id', 'linkedin_user_name'],  #You can also specify a list of columns\n",
    "    abs_tol=0, #Optional, defaults to 0\n",
    "    rel_tol=0, #Optional, defaults to 0\n",
    "    df1_name='user_local', #Optional, defaults to 'df1'\n",
    "    df2_name='user_aws' #Optional, defaults to 'df2'\n",
    "    )\n",
    "compare.matches(ignore_extra_columns=False)\n",
    "# False\n",
    "\n",
    "# This method prints out a human-readable report summarizing and sampling differences\n",
    "print(compare.report())\n",
    "\n",
    "only_df1=compare.df1_unq_rows\n",
    "only_df2=compare.df2_unq_rows\n",
    "\n",
    "# only_rest.groupby('hour').count().plot()\n",
    "# only_wss.groupby('hour').count().plot()\n",
    "\n",
    "# only_rest_desc=only_rest.describe()\n",
    "# trade_socket_desc=only_wss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_applications2 = datasource_applications.withColumn(\"application_rank\", F.row_number().over(Window.partitionBy('linkedin_user_name').orderBy(F.col(\"application_creation_date_ts\").desc(), F.col(\"application_id_backoffice\").desc())))\\\n",
    ".withColumn(\"previous_applications_qty\", F.row_number().over(Window.partitionBy('linkedin_user_name').orderBy(F.col(\"application_creation_date_ts\").asc(), F.col(\"application_id_backoffice\").asc())))\\\n",
    ".withColumn(\"current_date\",F.current_date())\\\n",
    ".withColumn('days_since_last_application', ((F.datediff(F.col('current_date'), F.col('application_creation_date_ts')))))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------+------------+---------------------------+\n",
      "|linkedin_user_name|application_creation_date_ts|current_date|days_since_last_application|\n",
      "+------------------+----------------------------+------------+---------------------------+\n",
      "|pedro-martine     |2021-11-09 11:00:48.081     |2022-08-11  |275                        |\n",
      "+------------------+----------------------------+------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasource_applications2.where(F.col('linkedin_user_name')=='pedro-martine')\\\n",
    ".withColumn(\"current_date\",F.current_date())\\\n",
    ".withColumn('days_since_last_application', ((F.datediff(F.col('current_date'), F.col('application_creation_date_ts')))))\\\n",
    ".select('linkedin_user_name', 'application_creation_date_ts', 'current_date', 'days_since_last_application')\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linkedin_user_name_tmp</th>\n",
       "      <th>prospect_location</th>\n",
       "      <th>prospects_updated_at_date_ts</th>\n",
       "      <th>prospect_seniority</th>\n",
       "      <th>prospect_area</th>\n",
       "      <th>declared_seniority</th>\n",
       "      <th>prospect_location_country_tmp1</th>\n",
       "      <th>prospect_location_country_tmp2</th>\n",
       "      <th>prospect_location_country</th>\n",
       "      <th>prospect_location_state_tmp1</th>\n",
       "      <th>prospect_location_state_tmp2</th>\n",
       "      <th>prospect_location_state</th>\n",
       "      <th>prospect_location_region_tmp1</th>\n",
       "      <th>prospect_location_region</th>\n",
       "      <th>prospect_department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bruno-raphael-ferreira-de-souza</td>\n",
       "      <td>Santos, São Paulo, Brasil</td>\n",
       "      <td>2022-08-10 14:28:30.141</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Mid-level</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>Full-stack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            linkedin_user_name_tmp          prospect_location  \\\n",
       "0  bruno-raphael-ferreira-de-souza  Santos, São Paulo, Brasil   \n",
       "\n",
       "  prospects_updated_at_date_ts prospect_seniority prospect_area  \\\n",
       "0      2022-08-10 14:28:30.141             Senior   Engineering   \n",
       "\n",
       "  declared_seniority prospect_location_country_tmp1  \\\n",
       "0          Mid-level                         Brasil   \n",
       "\n",
       "  prospect_location_country_tmp2 prospect_location_country  \\\n",
       "0                         Brasil                    Brazil   \n",
       "\n",
       "  prospect_location_state_tmp1 prospect_location_state_tmp2  \\\n",
       "0                    São Paulo                    São Paulo   \n",
       "\n",
       "  prospect_location_state prospect_location_region_tmp1  \\\n",
       "0               São Paulo                     São Paulo   \n",
       "\n",
       "  prospect_location_region prospect_department  \n",
       "0                São Paulo          Full-stack  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasource_prospects2.where(F.col('linkedin_user_name')=='bruno-raphael-ferreira-de-souza').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_prospects2 = datasource_prospects\\\n",
    ".withColumn('prospect_location_country_tmp1', F.reverse(F.split(F.col('prospect_location'), ',')).getItem(0))\\\n",
    ".withColumn('prospect_location_country_tmp2', F.trim(F.regexp_replace(F.col('prospect_location_country_tmp1'), \" e Região\", \"\")))\\\n",
    ".withColumn('prospect_location_country', prospect_country_func(F.col('prospect_location_country_tmp2')))\\\n",
    ".withColumn('prospect_location_state_tmp1', F.reverse(F.split(F.col('prospect_location'), ',')).getItem(1))\\\n",
    ".withColumn('prospect_location_state_tmp2', F.trim(F.regexp_replace(F.col('prospect_location_state_tmp1'), \" e Região\", \"\")))\\\n",
    ".withColumn('prospect_location_state', prospect_region_international_func(F.col('prospect_location_state_tmp2'), F.col('prospect_location_country')))\\\n",
    ".withColumn('prospect_location_region_tmp1', prospect_region_func(F.col('prospect_location_state')))\\\n",
    ".withColumn('prospect_location_region', prospect_region_international_func(F.col('prospect_location_region_tmp1'), F.col('prospect_location_country')))\\\n",
    ".withColumn(\"prospect_department\", F.trim(F.split(F.col('prospect_area'),' - ').getItem(1)))\\\n",
    ".withColumn(\"prospect_area\", F.trim(F.split(F.col('prospect_area'),' ').getItem(0)))\\\n",
    ".withColumnRenamed('linkedin_user_name', 'linkedin_user_name_tmp')\\\n",
    ".withColumnRenamed('updated_at_date_ts', 'prospects_updated_at_date_ts')\\\n",
    ".fillna(value=\"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação das variáveis de pais com base na variavel propesct_location do linkedin. Para comparar abordagens de talentos que moram fora do Brasil\n",
    "def prospect_country(country):\n",
    "    if country in ('', 'null', None, 'None', 'Missing'):\n",
    "        return \"Missing\"\n",
    "    elif country == 'São Paulo':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Rio de Janeiro':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Campinas':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Belo Horizonte':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Porto Alegre':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Curitiba':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Brasília':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Florianópolis':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Salvador':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Fortaleza':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Recife':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Manaus':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Ribeirão Preto':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Goiânia':\n",
    "        return 'Brazil'\n",
    "    elif country == 'João Pessoa':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Londrina':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Vitória':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Cuiabá':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Greater São Paulo Area':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Natal':\n",
    "        return 'Brazil'\n",
    "    elif country == 'São luis':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Brazil':\n",
    "        return 'Brazil'\n",
    "    elif country == 'Brasil':\n",
    "        return 'Brazil'\n",
    "    else:\n",
    "        return 'Others'\n",
    "    \n",
    "prospect_country_func = F.udf(prospect_country, StringType())\n",
    "    \n",
    "#Separacao entre os estados brasileiros, os estados do norte e nordeste foram definidos como um unico grupo porque o volume de abordagens eh menor\n",
    "def prospect_region(state):\n",
    "    if state in ('', 'null', None, 'None', 'Missing'):\n",
    "        return \"Missing\"\n",
    "    elif state == 'São Paulo':\n",
    "        return 'São Paulo'\n",
    "    elif state == 'Minas Gerais':\n",
    "        return 'Minas Gerais'\n",
    "    elif state == 'Rio de Janeiro':\n",
    "        return 'Rio de Janeiro'\n",
    "    elif state == 'Paraná':\n",
    "        return 'Paraná'\n",
    "    elif state == 'Santa Catarina':\n",
    "        return 'Santa Catarina'\n",
    "    elif state == 'Rio Grande do Sul':\n",
    "        return 'Rio Grande do Sul'\n",
    "    elif state == 'Espírito Santo':\n",
    "        return 'Espírito Santo'\n",
    "    elif state == 'Mato Grosso do Sul':\n",
    "        return 'Mato Grosso do Sul'\n",
    "    elif state == 'Mato Grosso':\n",
    "        return 'Mato Grosso'    \n",
    "    elif state == 'Goiás':\n",
    "        return 'Goiás'\n",
    "    elif state == 'Paraíba':\n",
    "        return 'Paraíba'\n",
    "    elif state == 'Pernambuco':\n",
    "        return 'Pernambuco'\n",
    "    elif state == 'Bahia':\n",
    "        return 'Bahia'\n",
    "    elif state == 'Sergipe':\n",
    "        return 'Sergipe'\n",
    "    elif state == 'Alagoas':\n",
    "        return 'Alagoas'\n",
    "    elif state == 'Ceará':\n",
    "        return 'Ceará'\n",
    "    elif state == 'Maranhão':\n",
    "        return 'Maranhão'\n",
    "    elif state == 'Piau':\n",
    "        return 'Piau'\n",
    "    elif state == 'Rio Grande do Norte':\n",
    "        return 'Rio Grande do Norte'\n",
    "    elif state in ( 'Acre'\n",
    "                   ,'Amapá'\n",
    "                   ,'Amazonas'\n",
    "                   ,'Pará'\n",
    "                   ,'Rondônia'\n",
    "                   ,'Roraima'\n",
    "                   ,'Tocantins'):\n",
    "        return 'Norte'\n",
    "    else:\n",
    "        return 'Others'\n",
    "    \n",
    "prospect_region_func = F.udf(prospect_region, StringType())\n",
    "\n",
    "#Correção da variavel estado quando o pais nao for Brasil\n",
    "def prospect_region_international(state, country):\n",
    "    if country not in ('Brazil', 'Missing'):\n",
    "        return \"International\"\n",
    "    elif state in ('', 'null', None, 'None', 'Missing'):\n",
    "        return \"Missing\"\n",
    "    elif state == 'sao paulo':\n",
    "        return \"São Paulo\"\n",
    "    elif state == 'Federal District':\n",
    "        return \"Distrito Federal\"\n",
    "    else:\n",
    "        return state\n",
    "prospect_region_international_func = F.udf(prospect_region_international, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "from urllib.parse import unquote\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successeful connected to MySQL: Engine(mysql+pymysql://backoffice:***@backoffice.cavx7zegjg23.us-east-1.rds.amazonaws.com:3306/intera_database)\n"
     ]
    }
   ],
   "source": [
    "##Status of candidates\n",
    "#Definição de parâmetros:\n",
    "ENDPOINT=\"backoffice.cavx7zegjg23.us-east-1.rds.amazonaws.com\"\n",
    "PORT=\"3306\"\n",
    "USR=\"backoffice\"\n",
    "REGION=\"us-east-1\"\n",
    "DBNAME=\"intera_database\"\n",
    "token='Lksmva3q0jewf#'\n",
    "os.environ['LIBMYSQL_ENABLE_CLEARTEXT_PLUGIN'] = '1'\n",
    "\n",
    "try:    \n",
    "    db_connection_str = 'mysql+pymysql://%s:%s@%s:%s/%s' % (USR, token, ENDPOINT, PORT, DBNAME)\n",
    "    db_connection = sa.create_engine(db_connection_str)\n",
    "    print(\"Successeful connected to MySQL: {}\".format(db_connection))\n",
    "except Exception as e:\n",
    "    print(\"Database connection failed due to {}\".format(e))\n",
    "    \n",
    "##Query with All candidates in mysql\n",
    "query = \"\"\"SELECT distinct linkedin_username as linkedin_user_name, status as candidate_status, updated_at FROM candidato where linkedin_username is not null and status is not null;\"\"\"\n",
    "datasource_candidates = spark.createDataFrame(pd.read_sql(query, con=db_connection))\\\n",
    ".withColumn('status_rank', F.when(F.col('candidate_status')=='AVAILABLE', F.lit(0)).otherwise(1))\\\n",
    ".orderBy(['linkedin_user_name', 'status_rank', 'updated_at'], ascending=False)\\\n",
    ".dropDuplicates(subset = [\"linkedin_user_name\"]).drop('updated_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- linkedin_user_name: string (nullable = true)\n",
      " |-- candidate_status: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasource_candidates = spark.createDataFrame(pd.read_sql(query, con=db_connection))\n",
    "datasource_candidates.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------+\n",
      "|  linkedin_user_name|candidate_status|status_rank|\n",
      "+--------------------+----------------+-----------+\n",
      "|lucas-gimenez-dia...|          ACTIVE|          1|\n",
      "+--------------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasource_candidates.where(F.col('linkedin_user_name')=='lucas-gimenez-dias-457307142').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-------------------+-----------+\n",
      "|linkedin_user_name|candidate_status|         updated_at|status_rank|\n",
      "+------------------+----------------+-------------------+-----------+\n",
      "|         ramonbrbs|       AVAILABLE|2021-09-28 20:19:00|          0|\n",
      "|         ramonbrbs|           HIRED|2021-09-16 09:14:44|          1|\n",
      "+------------------+----------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasource_candidates.where(F.col('linkedin_user_name')=='ramonbrbs')\\\n",
    ".withColumn('status_rank', F.when(F.col('candidate_status')=='AVAILABLE', F.lit(0)).otherwise(1))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
